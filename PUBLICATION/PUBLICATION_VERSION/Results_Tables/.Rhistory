# Load corpus from all txt files in given DirSource
corp_TM_doclevel <- Corpus(DirSource(directory = "/Users/kevin/Desktop/lm_testdata/filings",
recursive = T,
mode = "text"))
# alternatively, we can use the base::names() function:
ctmdl.names <- names(corp_TM_doclevel)
fun.replace_ext_abbrevs <- content_transformer(function(x, custlist_abbrevs) replace_abbreviation(x, abbreviation = custlist_abbrevs))
addit_abbrevs <- data.frame(abv = c("USD", "CHF", "EUR", "GBP", "e.g.", "CAD"),
rep = c("US Dollar", "Swiss Franc", "Euro", "British Pound", "eg", "Canadian Dollar"))
mycustlist.abbrevs <- rbind(qdapDictionaries::abbreviations, addit_abbrevs)
rm(addit_abbrevs)
# now transform:
corp_TM_doclevel_cleaned <- corp_TM_doclevel %>%
tm_map(., stripWhitespace) %>% # 1
tm_map(., content_transformer(replace_symbol)) %>% # 2
tm_map(., content_transformer(replace_abbreviation, mycustlist.abbrevs)) %>% # 3
tm_map(., content_transformer(replace_contraction)) %>% # 4
tm_map(., removePunctuation) %>% # 5
tm_map(., content_transformer(tolower)) %>% # 6
tm_map(., removeNumbers) %>% # 7
tm_map(., stemDocument) # 8
# Remove the LM lexica from environment
# except the lm_dict_allcombined and lm_dict_allcombined_uniques, which will be used for DTM subsettung
rm(list = setdiff(ls(), c("lm_dict_allcombined", "lm_dict_allcombined_uniques")))
# Load corpus from all txt files in given DirSource
corp_TM_doclevel <- Corpus(DirSource(directory = "/Users/kevin/Desktop/lm_testdata/filings",
recursive = T,
mode = "text"))
# alternatively, we can use the base::names() function:
ctmdl.names <- names(corp_TM_doclevel)
# for step (3): define additional abbreviations to be replaced, define function to replace
fun.replace_ext_abbrevs <- content_transformer(function(x, custlist_abbrevs) replace_abbreviation(x, abbreviation = custlist_abbrevs))
addit_abbrevs <- data.frame(abv = c("USD", "CHF", "EUR", "GBP", "e.g.", "CAD"),
rep = c("US Dollar", "Swiss Franc", "Euro", "British Pound", "eg", "Canadian Dollar"))
mycustlist.abbrevs <- rbind(qdapDictionaries::abbreviations, addit_abbrevs)
rm(addit_abbrevs)
# now transform:
corp_TM_doclevel_cleaned <- corp_TM_doclevel %>%
tm_map(., stripWhitespace) %>% # 1
tm_map(., content_transformer(replace_symbol)) %>% # 2
tm_map(., replace_abbreviation, mycustlist.abbrevs) %>% # 3
tm_map(., content_transformer(replace_contraction)) %>% # 4
tm_map(., removePunctuation) %>% # 5
tm_map(., content_transformer(tolower)) %>% # 6
tm_map(., removeNumbers) %>% # 7
tm_map(., stemDocument) # 8
# view doc afterwards:
writeLines(as.character(corp_TM_doclevel_cleaned[[1]]))
# Caution: applying those transformator loses meta information, most importantly the ID/names
# we can restore them -/somehow/- later on, using variable ctmdl.names saved manually above
# a simple re-assignment doesn't work, as object of type corpus does not seem to allow this
names(corp_TM_doclevel_cleaned)
# first off, define a list of words (which previously were symbols, but got converted using qdap::replace_symbol)
finterms_i_wanna_keep <- c("percent", "percentage", "dollar", "dollars", "euro",
"euros", "pound", "pounds", "yen", "franc", "francs", "usd")
# add senti category to LM list
lm_dict_allcombined <- lm_dict_allcombined %>% mutate(., senti_cat = NA)
lm_dict_allcombined$senti_cat[1:884] <- "negative"
lm_dict_allcombined$senti_cat[885:1029] <- "positive"
lm_dict_allcombined$senti_cat[1030:1158] <- "uncertain"
lm_dict_allcombined$senti_cat[1159:1609] <- "litigious"
lm_dict_allcombined$senti_cat[1610:1666] <- "constraining"
lm_dict_allcombined$senti_cat[1667:1683] <- "strongmodal"
lm_dict_allcombined$senti_cat[1684:1696] <- "modestmodal"
lm_dict_allcombined$senti_cat[1697:1714] <- "weakmodal"
is.na(lm_dict_allcombined$senti_cat) %>% sum(.)
# rename cols
colnames(lm_dict_allcombined) <- c("LM_tag", "word", "senti_cat")
lm_dict_allcombined
# combine LM list and finterms I wish to keep
fins_to_be_kept_tbl <- as_tibble(data.frame(LM_tag = rep("KT_FIN", length(finterms_i_wanna_keep)),
word = finterms_i_wanna_keep,
senti_cat = rep("KT", length(finterms_i_wanna_keep))))
tbl_TM_doclevel <- as_tibble(corp_TM_doclevel_cleaned) %>%
unnest_tokens(word, text, token = "words")
# count how often word W appears in the tibble
W <- "dollar"
tbl_TM_doclevel %>%
filter(., word == W) %>%
summarise(., counthowmany = n())
# count how often word W appears in the tibble
W <- "dollars"
tbl_TM_doclevel %>%
filter(., word == W) %>%
summarise(., counthowmany = n())
# count how often word W appears in the tibble
W <- "percent"
tbl_TM_doclevel %>%
filter(., word == W) %>%
summarise(., counthowmany = n())
# count how often word W appears in the tibble
W <- "percentage"
tbl_TM_doclevel %>%
filter(., word == W) %>%
summarise(., counthowmany = n())
# count how often word W appears in the tibble
W <- "percent"
tbl_TM_doclevel %>%
filter(., word == W) %>%
summarise(., counthowmany = n())
# count how often word W appears in the tibble
W <- "compan"
tbl_TM_doclevel %>%
filter(., word == W) %>%
summarise(., counthowmany = n())
# count how often word W appears in the tibble
W <- "company"
tbl_TM_doclevel %>%
filter(., word == W) %>%
summarise(., counthowmany = n())
# count how often word W appears in the tibble
W <- "draft"
tbl_TM_doclevel %>%
filter(., word == W) %>%
summarise(., counthowmany = n())
# count how often word W appears in the tibble
W <- "filing"
tbl_TM_doclevel %>%
filter(., word == W) %>%
summarise(., counthowmany = n())
# count how often word W appears in the tibble
W <- "fil"
tbl_TM_doclevel %>%
filter(., word == W) %>%
summarise(., counthowmany = n())
# count how often word W appears in the tibble
W <- "rep"
tbl_TM_doclevel %>%
filter(., word == W) %>%
summarise(., counthowmany = n())
# count how often word W appears in the tibble
W <- "dollar"
tbl_TM_doclevel %>%
filter(., word == W) %>%
summarise(., counthowmany = n())
# combine LM list and finterms I wish to keep
fins_to_be_kept_tbl <- as_tibble(data.frame(LM_tag = rep("KT_FIN", length(finterms_i_wanna_keep)),
word = finterms_i_wanna_keep,
senti_cat = rep("KT", length(finterms_i_wanna_keep))))
# keeping only those tokens that are in LM list (+ $, €, £ and %)
tbl_TM_doclevel <- as_tibble(corp_TM_doclevel_cleaned) %>%
unnest_tokens(word, text, token = "words") %>%
inner_join(., rbind(lm_dict_allcombined, fins_to_be_kept_tbl), by = "word")
# count how often word W appears in the tibble
W <- "dollar"
tbl_TM_doclevel %>%
filter(., word == W) %>%
summarise(., counthowmany = n())
# count how often word W appears in the tibble
W <- "usd"
tbl_TM_doclevel %>%
filter(., word == W) %>%
summarise(., counthowmany = n())
tbl_TM_doclevel <- as_tibble(corp_TM_doclevel_cleaned) %>%
unnest_tokens(word, text, token = "words")
# count how often word W appears in the tibble
W <- "usd"
tbl_TM_doclevel %>%
filter(., word == W) %>%
summarise(., counthowmany = n())
# count how often word W appears in the tibble
W <- "usdollar"
tbl_TM_doclevel %>%
filter(., word == W) %>%
summarise(., counthowmany = n())
# count how often word W appears in the tibble
W <- "dollar"
tbl_TM_doclevel %>%
filter(., word == W) %>%
summarise(., counthowmany = n())
# count how often word W appears in the tibble
W <- "chf"
tbl_TM_doclevel %>%
filter(., word == W) %>%
summarise(., counthowmany = n())
# count how often word W appears in the tibble
W <- "dollar"
tbl_TM_doclevel %>%
filter(., word == W) %>%
summarise(., counthowmany = n())
# keeping only those tokens that are in LM list (+ $, €, £ and %)
tbl_TM_doclevel <- as_tibble(corp_TM_doclevel_cleaned) %>%
unnest_tokens(word, text, token = "words") %>%
inner_join(., rbind(lm_dict_allcombined, fins_to_be_kept_tbl), by = "word")
# count how often word W appears in the tibble
W <- "dollar"
tbl_TM_doclevel %>%
filter(., word == W) %>%
summarise(., counthowmany = n())
# quick count/sort of most common words and most common categories
tbl_TM_doclevel %>% count(., word, sort = T)
tbl_TM_doclevel %>% count(., senti_cat, sort = T)
tbl_TM_doclevel %>% count(., word, senti_cat, sort = T)
# on full, initial corpus
dtm_TM_doclevel <- DocumentTermMatrix(corp_TM_doclevel_cleaned)
# summary stats about DTM, like no. of docs, no. of terms, sparsity, ...
dtm_TM_doclevel
# these figures can also access those separately:
Docs(dtm_TM_doclevel) # Note: retrieves the txtfilenames, similar to the 2 commands on corpus level above
# note the "unsorted" doc names without leading zeros
# to rename, transform -- final -- result below to a matrix and re-set its rownames()
# dtm_as_mat <- as.matrix(dtm_TM_doclevel)
# rownames(dtm_as_mat) <- ctmdl.names
nDocs(dtm_TM_doclevel)
Terms(dtm_TM_doclevel)
nTerms(dtm_TM_doclevel)
# inspect a small sample of the first 10 rows and some cols
inspect(dtm_TM_doclevel)
dtm_TM_doclevel[4:5,]
inspect(dtm_TM_doclevel[4:5,])
# can also use matrix indexing to subset what we seek for
inspect(dtm_TM_doclevel[4:5, 1:5])
# can also use matrix indexing to subset what we seek for
inspect(dtm_TM_doclevel[9:12, 1:10])
# can also use matrix indexing to subset what we seek for
inspect(dtm_TM_doclevel[3:8, 1:10])
# subset only those columns of the DTM that are in LM list or finterms, i.e. those in:
terms_tbkept <- rbind(lm_dict_allcombined, fins_to_be_kept_tbl)
terms_tbkept <- terms_tbkept$word
"%ni%" <- Negate("%in%") # creates function for "not in"
ncol(dtm_TM_doclevel)
sum(colnames(dtm_TM_doclevel) %in% terms_tbkept); sum(colnames(dtm_TM_doclevel) %ni% terms_tbkept)
sum(colnames(dtm_TM_doclevel) %in% terms_tbkept) + sum(colnames(dtm_TM_doclevel) %ni% terms_tbkept)
# subetting in standard matrix index notation
dtm_TM_doclevel_LM_only <- dtm_TM_doclevel[, colnames(dtm_TM_doclevel) %in% terms_tbkept]
ncol(dtm_TM_doclevel_LM_only)
inspect(dtm_TM_doclevel_LM_only)
# inspect the ones NOT in both lists
terms_tbkept %>% as_tibble(.) %>% arrange(value) # 1720
colnames(dtm_TM_doclevel_LM_only) %>% as_tibble(.) %>% arrange(value) # 557
# random check for a word
"addendum" %in% colnames(dtm_TM_doclevel_LM_only); "addendum" %in% terms_tbkept
"abl" %in% colnames(dtm_TM_doclevel_LM_only); "abl" %in% terms_tbkept
"will" %in% colnames(dtm_TM_doclevel_LM_only); "will" %in% terms_tbkept
# the ones not contained in the "full" DTM
not_in_initial_DTM <- anti_join(as_tibble(terms_tbkept),
as_tibble(colnames(dtm_TM_doclevel_LM_only)),
by = "value")
nrow(not_in_initial_DTM); not_in_initial_DTM %>% unique(.) %>% nrow(.)
a <- terms_tbkept %ni% colnames(dtm_TM_doclevel_LM_only)
sum(a)
rm(a, not_in_initial_DTM, W, fun.replace_ext_abbrevs, mycustlist.abbrevs, finterms_i_wanna_keep)
# rename doc names and export
dtm_as_mat <- as.matrix(dtm_TM_doclevel_LM_only)
rownames(dtm_as_mat) <- ctmdl.names
write.table(dtm_as_mat,
file = "/Users/kevin/Desktop/lm_testdata/DTM_LM_occurring-only.txt",
col.names = T, row.names = T,
sep = ";", quote = F)
# multiply each columns with weight scalar, given in vector
estimated_weights <- rep(0.123,
ncol(dtm_as_mat)) # 557 made up weights for 557 words
# using linear algebra:
# (1. create diagonal matrix from vector, 2. apply element-wise product)
wd <- diag(estimated_weights) # 1
weighted_dtm <- dtm_as_mat %*% wd # 2
# can apply rowsum to compute scores
scores <- rowSums(weighted_dtm) %>% as.data.frame()
colnames(scores) <- "score.all.LM"
# plot scores over docs (= time, as sequential)
ggplot(data = scores,
aes(x = 1:nrow(scores),
y = score.all.LM,
group = 1)) +
geom_area() + geom_point() +
scale_x_discrete(limits = seq(1, 12, 5)) +
scale_y_continuous(labels = comma) +
labs(title = "Scores for Filings (sorted by submission date)",
subtitle = "Scores were computed over full set of words in *all* LM lexica.",
x = "",
y = "Sentiment Score") +
theme_minimal()
# plot scores over docs (= time, as sequential)
ggplot(data = scores,
aes(x = 1:nrow(scores),
y = score.all.LM,
group = 1)) +
geom_bar() +
scale_x_discrete(limits = seq(1, 12, 5)) +
scale_y_continuous(labels = comma) +
labs(title = "Scores for Filings (sorted by submission date)",
subtitle = "Scores were computed over full set of words in *all* LM lexica.",
x = "",
y = "Sentiment Score") +
theme_minimal()
# plot scores over docs (= time, as sequential)
ggplot(data = scores,
aes(x = 1:nrow(scores),
y = score.all.LM,
group = 1)) +
geom_bar(stat = "bin") +
scale_x_discrete(limits = seq(1, 12, 5)) +
scale_y_continuous(labels = comma) +
labs(title = "Scores for Filings (sorted by submission date)",
subtitle = "Scores were computed over full set of words in *all* LM lexica.",
x = "",
y = "Sentiment Score") +
theme_minimal()
# plot scores over docs (= time, as sequential)
ggplot(data = scores,
aes(x = 1:nrow(scores),
y = score.all.LM,
group = 1)) +
geom_bar(stat = "count") +
scale_x_discrete(limits = seq(1, 12, 5)) +
scale_y_continuous(labels = comma) +
labs(title = "Scores for Filings (sorted by submission date)",
subtitle = "Scores were computed over full set of words in *all* LM lexica.",
x = "",
y = "Sentiment Score") +
theme_minimal()
x = score.all.LM,
group = 1)) +
geom_bar() +
scale_x_discrete(limits = seq(1, 12, 5)) +
scale_y_continuous(labels = comma) +
labs(title = "Scores for Filings (sorted by submission date)",
subtitle = "Scores were computed over full set of words in *all* LM lexica.",
x = "",
y = "Sentiment Score") +
theme_minimal()
x = score.all.LM,
group = 1)) +
geom_bar() +
#scale_x_discrete(limits = seq(1, 12, 5)) +
scale_y_continuous(labels = comma) +
labs(title = "Scores for Filings (sorted by submission date)",
subtitle = "Scores were computed over full set of words in *all* LM lexica.",
x = "",
y = "Sentiment Score") +
theme_minimal()
# plot scores over docs (= time, as sequential)
ggplot(data = scores,
aes(x = 1:nrow(scores),
y = score.all.LM,
group = 1)) +
geom_area() + geom_point() +
scale_x_discrete(limits = seq(1, 12, 5)) +
scale_y_continuous(labels = comma) +
labs(title = "Scores for Filings (sorted by submission date)",
subtitle = "Scores were computed over full set of words in *all* LM lexica.",
x = "",
y = "Sentiment Score") +
theme_minimal()
# -- DTM AS TIBBLE -- #
# can transform the (weighted) dtm into tidy format to continue working with it
tidy_dtm <- as_tibble(as.data.frame(dtm_as_mat))
rownames(tidy_dtm) # just to check that those are kept...
weighted_tidy_dtm <- as_tibble(as.data.frame(weighted_dtm))
rownames(weighted_tidy_dtm) # just to check that those are kept...
tidy_dtm
# rename doc names and export
dtm_as_mat <- as.matrix(dtm_TM_doclevel_LM_only)
dtm_as_mat_full <- as.matrix(dtm_TM_doclevel)
rownames(dtm_as_mat, dtm_as_mat_full) <- ctmdl.names
rownames(dtm_as_mat_full) <- ctmdl.names
dtm_as_mat_full
dtm_TM_doclevel
inspect(dtm_TM_doclevel[1:10, 500:510])
inspect(dtm_TM_doclevel[1:10, 1500:1510])
write.table(dtm_as_mat_full,
file = "/Users/kevin/Desktop/lm_testdata/DTM_all-orig-terms.txt",
col.names = T, row.names = T,
sep = ";", quote = F)
View(dtm_as_mat_full)
# Constructing the DTM from the "restricted", cleaned TIDY corpus
tbl_TM_doclevel_withcounts <- tbl_TM_doclevel %>%
count(., doc_id, word, sort = T) %>%
arrange(doc_id)
tbl_TM_doclevel_withcounts
tbl_TM_doclevel
# Constructing the DTM from the "restricted", cleaned TIDY corpus
tbl_TM_doclevel_withcounts <- tbl_TM_doclevel %>%
count(., doc_id, word, sort = T) %>%
arrange(doc_id)
tbl_TM_doclevel_withcounts
# using tidytext::cast_dtm() function
# it requires exactly the inputs like they are in my tbl_TM_doclevel_withcounts
# see: https://www.tidytextmining.com/dtm.html#cast-dtm
dtm_TM_doclevel_tidy <- tbl_TM_doclevel_withcounts %>% cast_tdm(word, doc_id, n)
save.image("~/Desktop/lm_testdata/TM_corps-and-DTMs_with-manips.RData")
# print textual content:
writeLines(as.character(corp_TM_doclevel[[1]]))
corp_TM_doclevel[[1]]
corp_TM_doclevel[[1]]$chars
corp_TM_doclevel[[1]]$content
corp_TM_doclevel[[1]]
corp_TM_doclevel[[1]]
length(corp_TM_doclevel[[1]])
length(corp_TM_doclevel[[1]]$content)
# print textual content:
writeLines(as.character(corp_TM_doclevel[[1]]))
length(as.character(corp_TM_doclevel[[1]]))
nchar(as.character(corp_TM_doclevel[[1]]))
# access single document in it using DOUBLE SQUARED BRACES (!)
# print generic info:
corp_TM_doclevel[[1]]
as.character(corp_TM_doclevel[[1]]) %>% nchar()
corp_TM_doclevel[[1]] %>% nchar()
as.character(corp_TM_doclevel[[1]]) %>% nchar()
#
nchar("asd")
#
nchar("asd bcd efg. One more...")
as.character(corp_TM_doclevel[[1]]) %>% ntoken()
as.character(corp_TM_doclevel[[1]]) %>% ntype()
as.character(corp_TM_doclevel[[1]]) %>% nsentence()
#
nsentence("asd bcd efg. One more...")
#
ntoken("asd bcd efg. One one one more...")
#
ntype("asd bcd efg. One one one more...")
#
ntype("asd bcd efg. One one one more.")
#
ntoken("asd bcd efg. One one one more.")
as.character(corp_TM_doclevel[[1]]) %>% ntype() %>% distinct()
as.character(corp_TM_doclevel[[1]]) %>% distinct()
sum(dtm_as_mat_full[1,])
sum(dtm_as_mat_full[1,]); sum(dtm_as_mat[1,])
sum(dtm_as_mat_full[1,] != 0); sum(dtm_as_mat[1,] != 0)
corp_TM_doclevel %>% ntoken()
as.character(corp_TM_doclevel) %>% ntoken()
typeof(corp_TM_doclevel)
typeof(corp_TM_doclevel[])
typeof(corp_TM_doclevel[[]])
lapply(corp_TM_doclevel, ntoken)
# Extract doc-length variables
as.character(corp_TM_doclevel[[1]]) %>% ntoken()
a <- lapply(corp_TM_doclevel, ntoken)
a
unlist(a)
doclengthvars$nb.types <- lapply(corp_TM_doclevel, ntype)
doclengthvars <- data.frame(txtfilename = ctmdl.names,
nb.tokens = NA, # based on corpus
nb.types = NA, # based on corpus
nb.sentences = NA, # based on corpus
nonzeroes.in.full.DTM = NA, # based on DTM
nonzeroes.in.existent.LM.DTM = NA) # based on DTM
doclengthvars
doclengthvars$nb.types <- lapply(corp_TM_doclevel, ntype)
doclengthvars
# Extract cols 2,3,4 using quanteda functions
doclengthvars$nb.types <- lapply(corp_TM_doclevel, nsentence)
# Extract cols 2,3,4 using quanteda functions
doclengthvars$nb.tokens <- lapply(corp_TM_doclevel, ntoken)
doclengthvars$nb.types <- lapply(corp_TM_doclevel, ntype)
doclengthvars$nb.sentences <- lapply(corp_TM_doclevel, nsentence)
doclengthvars
rowsum(dtm_as_mat_full)
dtm_as_mat_full
rowSums(dtm_as_mat_full)
dtm_as_mat_full != 0
View(dtm_as_mat_full != 0)
# Extraction length variables ---------------------------------------------
# Extract doc-length variables from corpus, leave 4 cols for DTM related length measures
doclengthvars <- data.frame(txtfilename = ctmdl.names,
nb.tokens = NA, # based on corpus
nb.types = NA, # based on corpus
nb.sentences = NA, # based on corpus
full.DTM.count = NA, # based on DTM
full.DTM.nonzeroes = NA, # based on DTM
LM.DTM.counts = NA, # based on DTM
LM.DTM.nonzeroes = NA) # based on DTM
# Extract cols 2,3,4 using quanteda functions
doclengthvars$nb.tokens <- lapply(corp_TM_doclevel, ntoken)
doclengthvars$nb.types <- lapply(corp_TM_doclevel, ntype)
doclengthvars$nb.sentences <- lapply(corp_TM_doclevel, nsentence)
doclengthvars
# doc-lengths based on DTM counts (already stemmed (and) LM-related)
doclengthvars$full.DTM.count <- rowSums(dtm_as_mat_full)
doclengthvars$full.DTM.nonzeroes <- rowSums(dtm_as_mat_full != 0)
doclengthvars$LM.DTM.count <- rowSums(dtm_as_mat)
doclengthvars$LM.DTM.nonzeroes <- rowSums(dtm_as_mat != 0)
doclengthvars
View(doclengthvars)
# Extraction length variables ---------------------------------------------
# Extract doc-length variables from corpus, leave 4 cols for DTM related length measures
doclengthvars <- data.frame(txtfilename = ctmdl.names,
nb.tokens = NA, # based on corpus
nb.types = NA, # based on corpus
nb.sentences = NA, # based on corpus
full.DTM.counts = NA, # based on DTM
full.DTM.nonzeroes = NA, # based on DTM
LM.DTM.counts = NA, # based on DTM
LM.DTM.nonzeroes = NA) # based on DTM
doclengthvars$nb.tokens <- lapply(corp_TM_doclevel, ntoken)
doclengthvars$nb.types <- lapply(corp_TM_doclevel, ntype)
doclengthvars$nb.sentences <- lapply(corp_TM_doclevel, nsentence)
doclengthvars$full.DTM.counts <- rowSums(dtm_as_mat_full)
doclengthvars$full.DTM.nonzeroes <- rowSums(dtm_as_mat_full != 0)
doclengthvars$LM.DTM.counts <- rowSums(dtm_as_mat)
doclengthvars$LM.DTM.nonzeroes <- rowSums(dtm_as_mat != 0)
View(doclengthvars)
# Export length-related variables (NB: last 2 cols not really senseful to use)
write.table(doclengthvars,
file = "/Users/kevin/Desktop/lm_testdata/doc-length-variables.txt",
col.names = T, row.names = F, sep = ";", quote = F)
# Export length-related variables (NB: last 2 cols not really senseful to use)
write.table(doclengthvars,
file = "/Users/kevin/Desktop/lm_testdata/doc_length_variables.txt",
col.names = T, row.names = F, sep = ";", quote = F)
# Export length-related variables (NB: last 2 cols not really senseful to use)
write.table(as.data.frame(doclengthvars),
file = "/Users/kevin/Desktop/lm_testdata/doc_length_variables.txt",
col.names = T, row.names = F, sep = ";", quote = F)
str(doclengthvars)
# Export length-related variables (NB: last 2 cols not really senseful to use)
write.table(as.matrix(doclengthvars),
file = "/Users/kevin/Desktop/lm_testdata/doc_length_variables.txt",
col.names = T, row.names = F, sep = ";", quote = F)
