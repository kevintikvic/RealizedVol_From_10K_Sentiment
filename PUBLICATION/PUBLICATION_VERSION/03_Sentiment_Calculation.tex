\section{Term Weighting and Calculation of Sentiment Scores}
\label{sec: sentiment_calcs}
This section outlines the first part of my research framework, namely the sentiment extraction process from the 10-K* filings. Starting with simple term counts and conventional weighting schemes, in the course of this section, I will also introduce the \enquote{volatility-impact-based} term weighting building on \textcite{Jegadeesh2013}; in this context, the volatility measure applied in this thesis will be explained. Moreover, the section also describes the aggregation of (weighted) sentiment frequencies into sentiment scores.

% ----------------------------------------------------------- %

\subsection{Term Counts, Document-Term-Matrix, and Vector Space Model}
\label{ssec: senti_counts}
\nomenclature{TDM}{Term-Document-Matrix}
\nomenclature{DTM}{Document-Term-Matrix}
\nomenclature{BoW}{Bag of Words}
% Circumvent the problem of Latex to not put % sign in an URL in a footnote by declaring "myurl"
\urldef\myurl\url{https://sraf.nd.edu/textual-analysis/resources/#Master%20Dictionary}

When considering sentiment analysis, a common starting point is the representation of the textual corpus $\mathcal{D}$ in a so-called document-term-matrix, commonly abbreviated by DTM\footnote{One can, alternatively, also transpose the matrix and represent the corpus as a term-document-matrix, TDM.}. As indicated, a corpus, denoted by $\mathcal{D}$, is simply a collection of $N$ different documents which the researcher seeks to analyze, i.e., $\mathcal{D}  = \{d_1, d_2, \dots, d_N \}$. Thus, in a DTM each of the rows of the DTM is representing one document $d_i$, $i = 1, \dots, N$, from corpus $\mathcal{D}$. In the case of this thesis, this corresponds to one 10-K* filing per row. Similarly, each column stands for one word \textit{type} in the available vocabulary\footnote{Althoug henceforth the term \enquote{word} will be used synonymously to the phrase \enquote{word type}, it is at this stage useful to briefly present an important concept in text analysis: the distinction of word \textbf{type} and \textbf{token}. While types represent the number of \textit{distinct} words within a corpus, tokens are the total number of all words \parencite{Jurafsky_Draft_2017}. Very common phrases like \textsf{and}, which appear very often in the corpus, imply that there are many tokens for this word but in fact only one type. This also highlights the usefulness of a matrix representation, in which one can take account for multiple occurrences of a word type by displaying its \textit{count}.}. The vocabulary in turn can be either formed in a \enquote{naive} way, i.e., include all terms that appear in the full corpus at least once (in other words, in at least one document) or, alternatively, contain only pre-selected words from a given lexicon. In this thesis, I will use eight different lexica produced by \textcite{Loughran2011}. The main reasons for this are indicated in the enumerated list in the subsequent section \ref{ssec: DTM_dimensionality}, in which I discuss crucial elements in determining the size of the vocabulary. Henceforth I will indicate the corresponding LM-lexicon to which a word belongs by using sub- and superscript $k$, with $k \in \{N, P, U, L, C, SM, MM, WM\}$, which stand for negative, positive, uncertainty, litigious, constraining, strong modal, modest modal, and weak modal, respectively\footnote{All dictionaries are provided by \textcite{Loughran2011} (\myurl). The constraining as well as weak and modest modal word lists are used in the linguistic analysis only, but are unrelated to the research hypotheses presented in this thesis (see chapter \ref{sec: volamodel}).}. Details on the LM word lists and their composition can be found in Appendix \ref{sec: annex_lm-lists}. The dictionary of category $k$, denoted by $\mathcal{J}_k$, has length $J_k$, i.e., consists of $J_k$ different words labelled $v_{1,k}, v_{2,k}, \dots, v_{J_{k},k}$. Thus, one can write the resulting eight DTM's in the following general notation:
\begin{equation}
DTM_k = 
\bordermatrix{
  & v_{1,k}	& v_{2,k}   & \dots   & v_{J_{k},k} \cr
d_1 & tf_{1,1}^{k} & tf_{1,2}^{k} & \dots & tf_{1,J_{k}}^{k} \cr
d_2 & tf_{2,1}^{k} & tf_{2,2}^{k} & \dots & tf_{2,J_{k}}^{k} \cr
\vdots & \vdots & \vdots & \ddots & \vdots \cr
d_N & tf_{N,1}^{k} & tf_{N,2}^{k} & \dots & tf_{N,J_{k}}^{k} \cr
}
\end{equation}

As said, $DTM_k$ has dimension $N \times J_k$, where the row dimension, $N$, represents the number of documents (i.e., 10-K* filings) in the corpus. The number of columns, $J_k$, denotes the number of sentiment words $v_{j,k}$, $j = 1, 2, \dots, J_{k}$, available in the LM dictionary of category $k$\footnote{One further remark can be made regarding the lexicon size: if one were to create one \enquote{global} DTM with all $J$ (stemmed) word types (coming from all eight available LM dictionaries), the dimensionality would not simply add up: Thus, $J = 1,574 < J_N + J_P + J_U + J_L + J_C + J_{SM} + J_{MM} + J_{WM} = 1,714$. The inequality comes from the fact that the sets $\mathcal{J}_k$ are not disjoint (i.e., several words occur in more than one list). For instance, the uncertainty and litigious sub-lexicon share some common words with the negative word list. Details regarding the LM lexica as well as their overlap are outlined in Appendix \ref{sec: annex_lm-lists}.}. Finally, and most importantly, each coefficient of the DTM, $tf_{i,j}^{k}$, with $i = 1, \dots, N$ and $j = 1, \dots, J_k$, denotes the number of occurrences of word type $j$ from LM-list $k$ in document $i$ (so-called raw \enquote{term frequency} (TF))\footnote{\label{fn: td_incidence_mat}There are also \enquote{simpler} variants of the DTM in which the coefficients do not represent the absolute term frequencies, but rather an indicator variable, i.e., a Boolean equalling one if the term appears (at least once) in the document, and zero else. This is often referred to as term-document \textbf{incidence} matrix \parencite[4]{ManningSchutze_IR_2008}.}. 

\nomenclature{TF}{Term Frequency}

Inspecting the rows of the DTM, one can see that each of the $N$ documents can be represented as an $J_k$-dimensional vector of word counts. Thus, this formulation is often referred to as \enquote{Vector Space Model} and is especially common in information retrieval applications, as it intuitively formalizes the idea that two similar documents will tend to contain comparable sets of words. Hence, their word count vectors should also be similar\footnote{The general task in information retrieval would be the detection of a set of documents, each represented by a $J_k$-dimensional document vector $\mathbf{D}$, that are most similar to a given set of search-/keywords, represented by a $J_k$-dimensional query vector $\mathbf{q}$. The similarity between two vectors in most applications is measured by so-called \textbf{cosine similarity}, i.e., the cosine of the angle that is spanned between the two vectors. Mathematically, one takes use of the notion of dot product and defines: 
\begin{equation*}
\cos(\theta) = \dfrac{\mathbf{D} \cdot \mathbf{q}}{\lVert \mathbf{D} \rVert \cdot \lVert \mathbf{q} \rVert} = \dfrac{\sum_{j=1}^{J_k} D_j q_j}{\sqrt{\sum_{j=1}^{J_k} D_j^2} \sqrt{\sum_{j=1}^{J_k} q_j^2}},
\end{equation*}
where $\lVert \mathbf{z} \rVert$ is called the \textbf{Euclidian norm} (commonly referred to as \enquote{length}) of the vector $\mathbf{z}$ and is used \enquote{normalize} vectors of differing lengths. Generally, $\cos(\theta) = -1$ for vectors pointing in opposite directions, $\cos(\theta) = 0$ for orthogonal vectors, and $\cos(\theta) = 1$ for vectors pointing in the same direction. However, as word counts are always non-negative ($tf_{i,j}^{k} \geq 0$), cosine similarity in text classification problems always lies between zero and unity \parencite[280]{Jurafsky_Draft_2017}.}. However, it is of crucial importance to note an underlying assumption that is implicitly made, if one seeks to represent the corpus in a DTM: if all documents can be displayed as vectors of term counts, the ordering of the words within the document is lost. This is commonly referred to as \textbf{bag-of-words} assumption; specifically, one represents each document as \enquote{an unordered set of words with their position ignored, keeping only their frequency in the document} \parencite[76]{Jurafsky_Draft_2017}. Therefore, the two sentences \textsf{Martin is faster than Tom} and \textsf{Tom is faster than Martin} have two equivalent bag-of-words vector representations, although their meaning is oppositional. 

Besides the implicit bag-of-word assumption, another common feature of vector spaces, also due to the often very high dimensionality of the DTM, is the so-called \emph{sparsity}, which refers to the fact that most of the elements within the vectors will be zero \parencite[252]{Jurafsky_Draft_2017}. This circumstance stems from the following observation: for a word $j$ to be included in the DTM, it is sufficient that the $j$-th column sum of the DTM is strictly greater than zero; however, this condition does not prevent many row sums to be zero. Thus, it is enough for the word to appear in only one of the documents to be part of the DTM, thereby creating a lot of zero entries for all other documents where it does \textbf{not} appear in. Especially (but not exclusively) in text \textit{classification} tasks the sparsity of the DTM can be problematic\footnote{The problem with sparsity arises most often in cases where the training (in-sample) data contains a zero count for a particular word \textit{and} class, whereas the term occurs in the test set (out-of-sample) for the first time in that specific class. Many (probabilistic) classifiers will at least in some form be based on the prior (conditional) probability (i.e., observed frequency) of the class given the term, which will be zero; thus underestimating the probability of occurrence. Note that this is not to be confused with the case where terms are non-existent in either class (so-called \enquote{unknown} or \enquote{out-of-vocabulary} (OOV) words), which in most cases are either fully omitted from the analysis or, alternatively, are added to the word list with an additional suffix tag like \texttt{UNK} (for unknown) \parencite[46, 79]{Jurafsky_Draft_2017}.}. Due to this reason, researchers often apply so-called \emph{smoothing} (or discounting) algorithms to the DTM. The most common choices are Laplace and \enquote{add-k} smoothing. The former simply replaces $tf_{i,j}^{k}$ in the DTM by $tf_{i,j}^{k} + 1$, i.e., increases all term counts by one. Similarly, the \enquote{add-k} smoother fills the cells of the TDM with $tf_{i,j}^{k} + k$ with $k \in (0,1)$ \parencite[47\psqq]{Jurafsky_Draft_2017}.

\nomenclature{OOV}{Out-Of-Vocabulary (Word)}

% ----------------------------------------------------------- %

\subsection{Dimensionality of the Document-Term-Matrix}
\label{ssec: DTM_dimensionality}

In discussing the DTM in general (and also with respect to the research specification in this thesis), a few other aspects are crucial, as they significantly affect the row dimension ($J_k$) of the matrix:
\begin{enumerate}[(1)]
\item \textbf{Choice of the vocabulary.} As already indicated, in this thesis the row dimension of $DTM_k$ will be equal to the number of all unique, stemmed words in sentiment list $k$. The other common choice is to let the row dimension be equal to \textbf{all} words that are available in the entire (training) corpus. This number will, in virtually all practical cases, massively exceed the number of LM-sentiment words\footnote{\textcite[274]{Jurafsky_Draft_2017} report that this number usually is somewhere between 10,000 and 50,000 words and thus much larger than the length of any meaningful sentiment lexicon.}. Consequently, reducing the analysis to a small subset of words in the corpus that one deems to carry \enquote{sentiment influence} might seem like a severe restriction. However, \textcite{TsaiWang2016} have extensively compared vector spaces using \enquote{original} versus \enquote{sentiment-limited} dimension and have shown that the latter perform (at least) equally good\footnote{It shall be highlighted that this methodology remains tested solely under the bag of words assumption. Moreover, many papers in existent literature do choose the same approach; however, many expand the LM-dictionary by also including the top-related terms in the total corpus-wordlist (determined by the usage of cosine similarity of such words to those in the respective LM list). Examples of this technique include \textcite{TsaiWang2014} and \textcite{Rekabsaz2017}.}. Regarding the choice of full versus pre-selected word list, also \textcite[1215]{LM-meta-2016} speak in favour of using pre-defined lexica by pointing out that using the full list of corpus words \enquote{can produce a relatively long list that is substantially and meaningfully shortened by selecting only those tokens that map into a list of words.} This is mainly due to the fact that many words in the original, full word-list are \enquote{meaningless} for sentiment analysis purposes. At the same time, this dimensionality reduction critically eases computational effort and is therefore also chosen for the project in hand. %Moreover, using word lists which can be agreed to carry pre-defined sentiment, allows us to use the volatility-based weighting scheme, which I will develop on in chapter \ref{sssec: senti_sscores}, on term counts. 

\item \textbf{Stop words.} Stop words are very frequent words like \textsf{the, a, of, or, in, that, to, and, with} or \textsf{for}, which have very high word counts, yet are semantically \enquote{meaningless} \parencite{Gries_2009, Jurafsky_Draft_2017}. Very often one would therefore exclude such words from the DTM. The question of stop word exclusion partly relates to vocabulary choice described above: calculating sentiment scores based on pre-defined sentiment lists instead of the original full-corpus vocabulary, does eliminate the necessity of defining and removing stop words from the corpus, as most common stop words do \textbf{not} appear in either of the LM sentiment lists\footnote{In this context, \textcite[1206]{LM-meta-2016} state: \enquote{Given that most business applications of textual analysis focus on using word counts from sentiment categories, the elimination or special treatment of stop words is typically not necessary.}}. However, the choice whether to in- or exclude them in the DTM might be very important for \textit{other} purposes. For instance, measuring document length by some function of the number of types or tokens, will produce very different results depending on whether stop words are considered or not. Moreover, many measures of document readability are based on syllabification. Since stop words are often monosyllabic, the in-/exclusion of such words will drastically influence the readability measure. As a final example, one might also think of a potential overlooking of negation, as words like \textsf{not} or \textsf{no} are included in some stop word lists.

\item \textbf{Lemmatization and stemming.} The process of lemmatization refers to \enquote{the task of determining that two words have the same root, despite their surface differences}, whereas stemming \enquote{refers to a simpler version of lemmatization in which we mainly just strip the suffixes from the end of the word} \parencite[11, 25]{Jurafsky_Draft_2017}. An example for lemmatization is to replace words like \textsf{am}, \textsf{are}, \textsf{is}, \textsf{were}, or \textsf{was} with their common root (to) \textsf{be}. The impact on the DTM is evident; in this case the dimension is heavily affected (instead of five words, only a single lemma would be left appearing in the DTM). Similarly, an example for stemming replaces words such as \textsf{computer, compute, computed, computational}, or \textsf{computationally} with their shared stem \textsf{comput} and thereby reduces word-dimensionality in the DTM as well\footnote{\textcite{Jegadeesh2013} manually inflected the LM dictionaries and thereby reduced the number of sentiment words by around one third, namely from 353 (2337) to 122 (716) for positive (negative) words.}. The choice of an appropriate lemmatizer or stemming algorithm is of crucial importance, as both procedures can be error-prone: for example, a stemming algorithm might stem the word \textsf{presentation} to \textsf{present}, thereby mingling it with the noun \textsf{present} (like gift), which then in turn itself might be confused with a form of \textsf{present} tense/time. The most widely used stemming algorithm, which is also applied to the LM word lists used in this thesis project, is the \emph{Porter Stemmer}, which is very simple and efficient; yet one should note that, as \textcite[25, 26]{Jurafsky_Draft_2017} illustrate, it is still exposed to make mistakes in terms of both over- and under-generalization.

\item \textbf{Negation tagging.} Negation tagging refers to the process of identifying semantical negations by adding pre- or suffixes to words in case that they co-occur with typical negation phrases like \textsf{no, not} or \textsf{never}. For illustration purposes, one can consider the following two sentences: \textsf{The company did \textbf{not} report a loss for three consecutive years} versus \textsf{The company did report a loss for three consecutive years}. The inclusion of the bold-faced word \textsf{not} completely reverses the sentiment extracted from two otherwise identical sentences. Negation tagging would transform the first sentence to either \textsf{The company did\_not report a loss for three consecutive years}, \textsf{The company did report\_not a loss for three consecutive years} or even \textsf{The company did report\_not a\_not loss\_not for\_not three\_not consecutive\_not years\_not}, where in the last version the negation tag is suffixed to each word until the next punctuation mark appears \parencite[81]{Jurafsky_Draft_2017}. 

% FROM DAS 2014, page 173: "Das and Chen [2007] (...) introduced the notion of “negation tagging” into the literature."

In the context of negation tagging, the most critical issue that needs to be addressed is the choice of the \enquote{negation window} around the word under consideration, i.e., the size of the n-gram\footnote{The notation of a \textbf{n-gram} simply refers to a sequence of $n$ words. Similarly, the notation of unigram (\enquote{single word}), bigram (\enquote{word pairs}), or trigram (\enquote{word triplets}) refers to the most common choices of $n=1$, $n=2$, and $n=3$, respectively.} around the current word in which one looks for negation phrases. For example, in the sentence \textsf{The weather today was not rainy but rather sunny} applying a negation tag also to the word \textsf{sunny} would alter the meaning of the sentence twice. However, when inspecting the word \textsf{sunny}, the choice of the n-gram obviously determines \enquote{how far around} this specific word one would search for negation phrases. In this case the critical question is whether $n$ is large enough so as to capture the fourth word preceding \textsf{sunny} (which would be the negator, \textsf{not}). Additional complexity arises due to the fact that this n-gram can be designed in three ways, i.e., considering preceding and/or subsequent words. 

Yet even this trivial example shows that there is no generally acceptable optimal solution regarding the choice of the appropriate n-gram, as the \enquote{searching-for-negation-window} shifts along as one continues to read. Particularly, in this example one would likely be interested to tag the word \textsf{rainy} transforming it to \textsf{rainy\_not}. This implies that $n \geq 1$ and inclusion of preceding words is required. In order to keep unchanged the words \textsf{but} and \textsf{rather} -- assuming they were not classified as stop words in the first place and therefore removed -- one must set $n = 1$ or $n=2$, respectively. The word \textsf{sunny} would remain untagged even if one considers a trigram of preceding (!) words\footnote{Note that in this case it is pointed out that the trigram covers only leading and no lagging words, as it is not clear what follows \textit{after} the word \textsf{sunny}. For instance, if the next sentence reads \textsf{On \textbf{no} other days it was sunny anymore}, and the trigram is symmetrically around \textsf{sunny}, then the word would be again falsely tagged due to the lagging negator in the next sentence.}.

\item \textbf{Numerical characters.} It is also important to clarify how to deal with figures that occur in the text body of the document. Some researchers and practitioners exclusively focus their analysis on the textual contents within the corpus and thus decide to delete sequences of numerical characters. Others, however, choose to replace them by a common representative symbol, like the hash sign (\texttt{\#}). Yet, even those apparently simple parsing procedures have their caveats. One might consider, for instance, a price tag of \mbox{\textsf{99.99 USD}} appearing in one of the documents. If the parsing algorithm mistakes the decimal separator (\texttt{.}) to be a sentence separator, this string would be replaced by \mbox{\texttt{\#}.\texttt{\#} \textsf{USD}}, failing to recognize that it is in fact a single number. Therefore, the \enquote{word} \texttt{\#} would falsely appear twice (instead of once) in the DTM. Similarly, contingent on how the parser deals with the \texttt{/} sign, a date string like \textsf{05/23/2009} might be transformed into three hashes, although it really refers to one single date. Common solutions to such problems include the usage of so-called \emph{regular expressions} (RegEx), which allow to search for detailed patterns in strings\footnote{In this case, in order to correctly classify a date, one would look for strings with the following pattern: one or two digits, followed by \texttt{/}, followed by one or two digits, followed by \texttt{/}, followed by four digits. It is easy to verify how such a RegEx would expose the researcher to the risk that dates might as well be formatted as \texttt{YYYY/MM/DD} or could, alternatively, be separated by the symbols \texttt{.} or \texttt{-} instead.}. Moreover, another problem in replacing numbers with the hash sign includes that the numerical count might be upward biased simply due to the presence of page numbers in the footer, consecutively numbered section headers, or the usage of enumerated instead of bullet-pointed lists. 
\end{enumerate}

\nomenclature{RegEx}{Regular Expression}

To address these issues, I will apply the following parsing procedure to the 10-K* sample used in this thesis:
\begin{enumerate}[(1)]
\item As indicated, I will extract one $DTM_k$ for each LM-lexicon $k$, thereby capturing 1,562 words out of the full (stemmed) LM word list with 1,574 unique terms (see Appendix \ref{sec: annex_lm-lists} for details). 
\item Using only words in the respective LM-category $k$, only six stop words appear in any of the DTM's: \textsf{against, could, further, ought, should, would}\footnote{The small overlap of the stop word list and the eight LM dictionaries is based on the stop word list provided by the \texttt{tm} package available for the programming language \texttt{R}, in which this research project was conducted. However, using other common stop word lists delivers highly similar results.}.
\item The words in the LM lists were stemmed using the Porter stemmer embedded in the \texttt{tm} package in \texttt{R}. The full lexicon of stemmed words is provided in Appendix \ref{sec: annex_lm-lists}. 
\item Due to the complexity attached to negation tagging, missing evidence from the literature regarding the size of the n-gram window as well as non-existence of negation word lists for financial corpora, no negation tagging was applied in this thesis. As \textcite[1217]{LM-meta-2016} point out, negation typically occurs with positive words, as the management of the filing company seeks to disguise bad messages using positive language; conversely, the authors state that \enquote{negative words seem unambiguous -- rarely does management negate a negative word to make a positive statement}, thereby inducing enhanced caution when interpreting results based on positive sentiment. 
% --> in LM2016 p.31 meta he even says leave your fingers from pos. list (except for JW nobody has shown them to be predictive anyways) --> mention that vibtw could outweigh potential positive words that tend to co-occur with negators
\item As the focus of this work is to investigate the impact of textual contents within corporate filings, numerical strings are removed from the corpus. Moreover, any variable constructed using counts of numeric characters would likely be subject to measurement error, as parsing can prove difficult for finance corpora. However, partly acknowledging the importance of messages conveyed by numbers, ratios, and statistics, I will capture the degree of \enquote{numerical information} by the number of occurrence of frequent companions as \textsf{percent}, \textsf{dollar}, or \textsf{Euro} (for details see section \ref{ssec: volamodel_vola+senti} on variable construction). 
\end{enumerate}

% ----------------------------------------------------------- %

\subsection{Transforming Term Counts to Term Weights}
\label{ssec: senti_termweighting}
Sentiment analysis literature, again borrowing from concepts in information retrieval, quickly realized that \textit{raw} term counts in the DTM suffer from significant drawbacks.

Firstly, term frequencies scale with document length. This implies that longer documents will in most cases also have higher term counts\footnote{\textcite[127]{ManningSchutze_IR_2008} provide an intuitive example for this fact, namely when a document $d_i$ is simply cloned and combined with itself. Raw term frequencies in this case double, although the \enquote{informational content} of the document after this manipulation is the same as before.}. Therefore, it is common to opt for the use of \textbf{relative} term frequencies instead of absolute term frequencies, implying that each count is scaled by document length ($L_{i,k} = \sum_{j=1}^{J_{k}} tf_{i,j}^k$)\footnote{One could, alternatively, also define document length in many other ways: For instance, one could simply count all number of tokens or types in the document. Similarly, one could also use the count of \textbf{all} sentiment words ($L_{i}$) instead of using category $k$-specific lengths. However, I find this approach more suitable for various reasons: Firstly, it assures that each row in each $DTM_k$ sums to unity. Secondly, using $L_{i,k}$ expresses the relative frequency within a group of similar words and thereby mitigates a potential dilution of the relative frequency in category $k_1$ due to high occurrences of words from category $k_2$. Thirdly, as I will compute sentiment-category specific scores as well, I prefer to operate with a category-specific length measure.}, thereby obtaining $rf_{i,j}^k = tf_{i,j}^k / L_{i,k} = tf_{i,j}^k / \sum_{j=1}^{J_{k}} tf_{i,j}^k$. This approach was, for instance, used in \textcite{TsaiWang2014} and \textcite{TsaiWang2016}. In some occasions, also a simple Boolean indicating occurrence versus non-occurrence of the term -- without considering the \enquote{degree} of occurrence -- might mitigate the length-scaling problem of raw term counts (see footnote \ref{fn: td_incidence_mat} on term-incidence-matrices).

Secondly, a comparison that is based solely on raw frequency implies that a word $v_{j_1,k}$ with $tf_{i,j_1}^k = 10$ is \textit{ten times} more important than a word $v_{j_2,k}$ with $tf_{i,j_2}^k= 1$. In fact, it is often argued that actually the more seldom / unique words are \enquote{more important}, as they help the reader to distinguish one specific document, in which they appear, from the other files in the corpus. For this reason, many studies apply a smoothing correction to raw term frequencies and use logarithms as transformation. The modified counts are often called \enquote{weighted} frequencies:
\begin{equation} \label{eq: wf}
wf_{i,j}^k = \begin{cases}
1 + \ln (tf_{i,j}^k) & \text{if } tf_{i,j}^k > 0 \\
0 & \, \text{else}
\end{cases}
\end{equation}
Sometimes, also $wf_{i,j}^k = \ln (1 + tf_{i,j}^k)$ is used so as to smooth the positive term counts (e.g., in \textcite{Rekabsaz2017}). I will refer to these weighting schemes as \texttt{WF\_1PLOG} and \texttt{WF\_LOG1P},  respectively, and I will use them as benchmark weighting schemes in robustness checks. 

Yet another alternative and commonly used weighting scheme is to  normalize all term counts by the count of the most common word in the respective document, i.e., the largest term frequency observed in the respective document $d_i$. Denoting the latter by $tf_{i, j_{max}}^k$, one can scale as follows:
 \begin{equation}
 maxtf_{v;d} = a + (1-a) \dfrac{ tf_{i,j}^k }{  tf_{i, j_{max}}^k   },
\end{equation}
where $a$ is a smoothing parameter that is usually set to .4 by convention \parencite[127]{ManningSchutze_IR_2008}. In other words, normalized term frequencies are obtained by scaling all elements in the DTM with the maximum term frequency of each respective row of the DTM in which they appear \parencite[126-127]{ManningSchutze_IR_2008}. In robustness checks, I will label this weighting approach \texttt{TFMAX}. 

%The last alternative that will be considered in this work is a slight modification of \texttt{TFMAX}; it can be specified by using the respective \textit{average} word count per row instead of the maximum word count (e.g., in \textcite[1208]{LM-meta-2016}). Hence, the weighting scheme is defined as:
% \begin{equation}
% avgtf_{v;d} = \dfrac{ tf_{i,j}^k }{  tf_{i, j_{avg}}^k   },
%\end{equation}
%where $tf_{i, j_{avg}}^k = 1/J_k \sum_{j=1}^{J_{k}} tf_{i,j}^k$ is the average term frequency in document $i$. This weighting scheme will henceforth be labelled \texttt{TFAVG}.

The third and most critical issue of applying raw/relative/weighted term frequencies stems from the following fact: as they are based on term counts \textit{within} a single document $i$, two sentiment words with equal frequency are considered equally important, regardless of how often they appear in \textbf{other} documents. This notion is critical, as one can illustrate with a brief example: the two negative LM words \textsf{defer} and \textsf{cyberattack} might have term counts of, say, 20 each in a given document $d_m$. However, assume that \textsf{defer} appears in \textit{all} other documents $d_i$ (with $i = 1, 2, \dots, N$)  of the corpus as well (for instance, due to usage of common phrases like \textsf{deferred tax asset/liability}); while \textsf{cyberattack} appears exclusively in $d_m$. It is obvious that the term \textsf{cyberattack} will have much more \enquote{explanatory power} in expressing a sentiment in that specific document $d_m$, although it has the exact same term frequency as \textsf{defer}. In contrast, the latter offers no \enquote{discriminatory} power whatsoever in order to distinguish document $d_m$ from all the others in the corpus. This example undermines why this issue was first addressed in information retrieval tasks, where distinction of documents is crucial. However, also sentiment analysis commonly borrows from the solution that was proposed to solve this issue, which is presented in the following subsection. 

\subsubsection{Term Weighting Schemes Using Inverse Document Frequency}
\label{sssec: senti_termweighting_old}

The most common choice to account for the differing occurrence of words within the whole corpus is to scale (raw or weighted) term counts with a measure called \textbf{inverse document frequency} (IDF) and create a combined measure called TF-IDF. Inverse document frequency attaches high weights to words that appear in few documents in the corpus, and low weights for words that are very common across documents in the collection. Formally,
\begin{equation} \label{eq: idf}
idf_{j,k} = \ln \left(\dfrac{N}{df_{j,k}} \right),
\end{equation}

where $N$, as usual, stands for the number of documents in the corpus and ${df_{j,k}}$ denotes the number of documents in which term $v_{j,k}$ appears at least once. In other words, ${df_{j,k}}$ corresponds to the number of non-zero entries for each \textbf{column} of the DTM. Note how, therefore, $idf_{j,k}$ has no document subscript $i$: being based on the columns of the DTM only, it is not a document-specific but rather a word-to-corpus-based measure. Moreover, and from a practical perspective, there is no consensus about which base of the logarithm is most appropriate (in both equations \eqref{eq: wf} and \eqref{eq: idf}), while most applications opt for either natural base, $\log_2$ or $\log_{10}$\footnote{In all subsequent applications in this thesis, natural logarithm will be used, while notations $\log$ and $\ln$ will be used interchangeably.}.

Using inverse document frequencies computed by equation \eqref{eq: idf}, the \enquote{combined} TF-IDF weight for term $v_{j,k}$ in document $i$ then becomes:
\begin{equation} \label{eq: tfidf}
tfidf_{i,j}^k = tf_{i,j}^k idf_{j,k}
\end{equation}
This weighting scheme will accordingly be named \texttt{TFIDF}.

If one instead opts to choose either form of the log-normalized variants for the term frequencies,  $wf_{i,j}^k$, the IDF-method accordingly defines:
\begin{equation} \label{eq: wfidf}
wfidf_{i,j}^k = wf_{i,j}^k idf_{j,k}
\end{equation}
Depending on the definition of $wf_{i,j}^k$ (using either $(1+\ln(x))$ or $\ln(1+x)$), this results in weighting schemes \texttt{WFIDF\_1PLOG} and \texttt{WFIDF\_LOG1P}, respectively. Similarly, one obtains \texttt{RFIDF} by:
\begin{equation} \label{eq: rfidf}
rfidf_{i,j}^k = rf_{i,j}^k idf_{j,k}
\end{equation}

As one can see from equations \eqref{eq: tfidf}, \eqref{eq: wfidf}, and \eqref{eq: rfidf}, the IDF-based weighting schemes favour those words that are frequent in one document $i$ but at the same time are rare in all other documents of the corpus. Hence, such weighting methods solve the problem described in the introductory example in section \ref{ssec: senti_termweighting}: by attributing to the \enquote{common word} \textsf{defer} a lower IDF value than to the \enquote{one-document-only} term \textsf{cyberattack}, the latter will, for equal term counts, have a higher IDF weight. For this reason, these weighting schemes are also a suitable method to detect and \enquote{filter out} stop words: it can be easily verified that for terms (such as \textsf{and} or \textsf{a}), which appear in \textit{all} documents, one obtains $df_{j,k}= N$ and therefore $tfidf_{i,j}^k= wfidf_{i,j}^k = rfidf_{i,j}^k = 0$. Thus, even if one fails to exclude the very common and \enquote{meaningless} words, an IDF-based scheme would weight those terms close to zero and \enquote{counterbalance} their high term counts. 

Generally, with respect to the distribution of words in a corpus, i.e., across documents, the most influential characterization comes from a model called \textbf{Zipf's law}, which states that each term's frequency is inversely related to its rank in the frequency table. In other words, the most common word appears twice as often as the second-most common, three times as often as the third-most common, and so on -- which implies that term frequencies within corpora exhibit a power law \parencite[89]{ManningSchutze_IR_2008}. 

\nomenclature{IDF}{Inverse Document Frequency}
\nomenclature{TF-IDF}{Term Frequency - Inverse Document Frequency}

Finally, to conclude this subsection, it is worth to reference the interested reader to \textcite{ZobelMoffat_termweights_1998}, who present other common weighting schemes (for instance, the Okapi BM25 function, which is very popular amongst practitioners).

\subsubsection{Weight Aggregation, Sentiment Scores, and Volatility-Impact-Based Term Weighting}
\label{sssec: senti_sscores}
\nomenclature{VIBTW}{Volatility-Impact-Based Term Weighting}
\nomenclature{PFRV}{Post-Filing Realized Volatility}

Having obtained weighted term frequencies, the next step is to aggregate the separate term counts/weights into a single document-wide score for each 10-K* filing. Following the approach described in \textcite{ManningSchutze_IR_2008} and adopted in a slightly modified fashion in \textcite{Jegadeesh2013}, the $k$-th LM-category score for document $i$ is calculated as a simple sum of the (weighted) term frequencies. For instance, using \texttt{TFIDF} results in the following score construction:
%\footnote{Recall that $k \in \{N, P, U, L, C, SM, MM, WM\}$.}
\begin{equation}
S_i^k(\texttt{TFIDF}) = \dfrac{1}{L_{i,k}} \sum_{j=1}^{J_{k}} tfidf_{i,j}^k, 
\end{equation}
where the first term scales the score by document length (denoted by $L_{i,k}$ and measured as element-wise sum of the $i$-th row in the $k$-th DTM). Replacing $tfidf_{i,j}^k$ with relative, log-transformed, or maximum-scaled counts, one can similarly obtain $S_i^k(\texttt{RF\_IDF})$, $S_i^k(\texttt{WF\_1PLOG})$, $S_i^k(\texttt{WF\_LOG1P})$, $S_i^k(\texttt{WFIDF\_1PLOG})$, $S_i^k(\texttt{WFIDF\_LOG1P})$, and $S_i^k(\texttt{TFMAX})$, respectively.

However, as \textcite[7]{Jegadeesh2013} point out, \enquote{although idf weights have an appeal in other contexts, there is no particular reason that the frequency of occurrence of a word in documents should be related to market's perception of its impact}. Therefore, they propose to estimate the score using a different, market-based adjustment of term frequencies. To illustrate this idea, it is helpful to recall the definition of \texttt{TFIDF}: $tfidf_{i,j}^k = tf_{i,j}^k idf_{j,k}$. This can be read as a simple two-component product of the term frequency and the IDF-weight for the corresponding term $j$. The idea of \textcite{Jegadeesh2013} basically seeks to replace the second part of this product: instead of using a corpus-based weight, one could alternatively find a market-based weight, that might prove more suitable in modelling market reactions to textual contents. As \textcite{Jegadeesh2013} modelled CAR subsequent to 10-K submissions, \enquote{market-based} weighting in their setting seeks to assign a higher weight to those words in the vocabulary, whose appearance in past 10-K documents led to higher post-filing CAR. \textcite{Jegadeesh2013} call these \textit{word power weights}. In a similar framework, in this thesis I will apply such a market-based term weighting scheme on the basis of the impact that words from sentiment \enquote{category} $k$ had on realized volatility \enquote{1-week} after submission\footnote{The definition of post-filing realized volatility, denoted by $\sigma_{t,i}^{PF}$ and henceforth abbreviated by \texttt{PFRV}, is described in detail in the subsequent section \ref{ssec: senti_LHS}.}. 

The next step is to estimate these market-based weights (henceforth labelled by $w_j$) in a linear regression framework based on a large number of in-sample, \enquote{training} 10-K* filings from the past. As such, it is useful to firstly define a sentiment score calculated on the basis of volatility-impact-based term weighting (\texttt{VIBTW}) in a similar fashion as it was achieved in section \ref{sssec: senti_termweighting_old}:
\begin{equation} \label{eq: vibtw_score}
S_i^k(\texttt{VIBTW}) = \dfrac{1}{L_{i,k}} \sum_{j=1}^{J_{k}} tf_{i,j}^k w_j
\end{equation}
Under the assumption that \texttt{VIBTW} scores linearly relate to post-filing realized volatility (denoted by \texttt{PFRV} or $\sigma_{t,i}^{PF}$), one can write and expand as follows:

\begin{align*}
\sigma_{t,i}^{PF} &= a + b S_i^k(\texttt{VIBTW}) + u_{t,i} \\
                             & = a + b  \left(      \dfrac{1}{L_{i,k}} \sum_{j=1}^{J_{k}} tf_{i,j}^k w_j              \right) + u_{t,i} \\ 
                             & = a + \left(  \sum_{j=1}^{J_{k}} rf_{i,j}^k   b  w_j \right) + u_{t,i} \\
 \numberthis  \label{eq: vib-regression}      & = a + \left(  \sum_{j=1}^{J_{k}} rf_{i,j}^k   B_j \right) + u_{t,i}
\end{align*}

where the notation indicates that document $i$ was filed at time $t$. The crucial element in \eqref{eq: vib-regression} is coefficient $B_j \equiv b \cdot w_j$. If this equation is estimated with OLS\footnote{Note that the summation index encompasses all words $j$ that are element of the respective LM dictionary $k$. For instance, for the negative word lexicon, this implies that equation \eqref{eq: vib-regression} will be a model with $J^N = 882$ regressors.}, one obtains estimates $\hat{B}_{j}$ instead of the weight estimates $\hat{w}_{j}$ which are seeked for, as the latter are scaled by the constant $b$. In order to attenuate potential distributional distortions arising from this, and following \textcite{Jegadeesh2013}, I will standardize (\enquote{z-transform}) the estimated coefficients so as to obtain estimated word weights:
\begin{equation} \label{eq: wv_estimates}
\hat{w}_{j} = \dfrac{\hat{B}_{j} - \bar{B}_{j}}{\text{Std.Dev.}(\hat{B}_{j})}, 
\end{equation}
with $\bar{B}_{j}$ being the mean estimate across all $J_k$ words and the denominator denoting the standard deviation of the $\hat{B}_{j}$'s around that mean. 

As indicated, the estimated $\hat{w}_{j}$ in equation \eqref{eq: wv_estimates} will be based on a subset of the available data, which is used to \enquote{learn} the volatility-based weighting scheme (i.e., a \textit{training} set). The obtained estimates will then be used in combination with out-of-sample term counts (which come from \enquote{upcoming} 10-K*s) so as to get scores $S_i^k(\texttt{VIBTW})$ for the 10-K* filings in this so-called \enquote{test} set. Mathematically, index $i$ in equation \eqref{eq: vib-regression} will \textbf{not} range from $1$ to $N$ but rather to $N_{1} < N$, where $N_1$ indicates the size of the training set. Similarly, the learned weights will be applied to out-of-sample 10-K* documents with $i = N_1+1, N_1+2, \dots, N$. For the ease of notation, I will denote the size of the test set as $N_2 = N-N_1$. 

In this context it is necessary to highlight that, due to the fact that estimated $\hat{w}_{j}$ instead of \enquote{true} weights ${w}_{j}$ are used, the score of newly examined filings, $S_i^k(\texttt{VIBTW})$, will be measured with error. However, as \textcite{Jegadeesh2013} describe, because of the high number of regressors embedded in the estimation of equation \eqref{eq: vib-regression} and \eqref{eq: wv_estimates}, the calculation of scores is not likely to be highly affected by this measurement error. In particular, individual miscalculations per word are likely to offset each other on an aggregated score-level, especially when estimation is performed on long(er) sample periods (i.e., large $N_1$)\footnote{In fact, \textcite[14, 15, 39]{Jegadeesh2013} examine the \enquote{stability} of scores using two/three non-overlapping sub-periods. Indeed, correlation between the scores is significant, implying they are \enquote{stable} over time. This indicates that variation in scores is likely to come from variation in the \enquote{true} scores rather than from measurement error.}.

\nomenclature{OLS}{Ordinary Least Squares}

\subsection{Measuring Post-Filing Volatility}
\label{ssec: senti_LHS}
\nomenclature{OHLC}{Open-High-Low-Close}
Using the impact on post-filing volatility to train the term weights in equation \eqref{eq: vib-regression}, one needs to define how this variable is measured. As volatility per se is latent, it is not an observable quantity; thus, one needs a suitable ex-post proxy variable \parencite{BrownleesEngleKelly2011}. The most common choices are realized volatility measures (for \textit{daily} data this imposes the additional requirement of availability of intra-day data but it is more tractable on a weekly time frame), squared returns (which have to be shown to be rather noisy, see \textcite{PattonSheppard2009}), or the (intra-day) range of log prices (loosely spoken, the difference between highest and lowest price). 

In this thesis, I will opt for a measure of weekly realized volatility in an event-study-like fashion, where the event window starts with the submission of the filing and ends within a single business week (i.e., four days) after the publication\footnote{Note that squared returns and absolute returns will be considered as alternative volatility proxies in robustness checks (section \ref{ssec: robust_alternative-vola-proxies}).}. In other words \texttt{PFRV} is measured as the (natural logarithm of) sample standard deviation of daily returns within the first $\tau$ days after the publication of the 10-K*. Analytically one can formulate as follows: 
\begin{equation} \label{eq: pfrv-definition}
\sigma_{t,i}^{PF}= \ln \left( \sqrt{\dfrac{1}{|\tau | - 1}   \sum_{s = t}^{\tau} \left(   R_{s,g} - \overline{R}_{[t,\tau],g}        \right)^2 } \right),
\end{equation}

where the notation suggests that report $i$ was filed by submitter $g$ at date $t$, $|\tau |$ is the size of the event window in days (here: $\tau = t + 4$ implying $|\tau | = 5$\footnote{The choice of the estimation window is motivated by two reasons: firstly, measuring daily volatility requires -- unless intra-day or tick data is available -- the use of a much noisier proxy of true volatility, such as squared returns. In this context \textcite[240]{EnglePatton_2001} highlight that longer sampling frequencies can (partly) circumvent this problem by using more reliable realized volatility measures. The second argument in favour of a slightly longer post-filing window is the fact that 10-K* reports are usually quite comprehensive and long. As it will be indicated in the data description (section \ref{sec: data_sample}) the median 10-K* has 49,117 words, implying that even institutional investors need some time to process the information contained in the filings properly. \textcite{Loughran2011} experimented in shrinking their event window for absolute excess returns and experienced the results becoming insignificant when the event window was narrowed to one or two days post-filing, respectively; leading them to conclude that \enquote{The document length would require the average investor some period of time to absorb the information} \parencite[53]{Loughran2011}. However, the choice of the post-filing window needs to be a balanced one; a too long post-filing window leaves larger room for other important events to occur, which could have substantial volatility impact and hence bias the relative importance of the (textual part of the) 10-K*.}), $R_{s,g}$ is the log-return of company $g$’s stock closing price on day $s$ (i.e., $R_{s,g} = \ln (P_s) - \ln (P_{s-1})$) and $\overline{R}_{[t,\tau],g} = 1/|\tau | \sum_{s = t}^{\tau} R_{s,g}$ is the average log return of the stock price of company $g$ in the time window $[t,\tau]$.

For the sake of completeness, it is worth mentioning that more sophisticated volatility proxies exist. For instance, \textcite{Raviv_online} presents three alternative volatility measures (usually thought to take use of intra-day data), that can also be employed for a weekly volatility measure if suitable data is available. They are \enquote{range-based} measures, that mainly use open-high-low-close (OHLC) data so as to capture the dispersion of price movement instead of solely relying on close-to-close (log-) returns\footnote{Interested readers are referred to \textcite{PattonSheppard2009} for a profound analysis of how imperfect volatility proxies affect the forecasting task and subsequent statistical and accuracy testing procedures.}. 

\clearpage