\section{Introduction}
\label{sec: introduction}
Consider the following situation: investor I is offered a choice between two alternatives. In the first scenario, a dice will be rolled and - depending on the number of eyes displayed - a certain amount will be paid to the investor. In case only one pip appears, she receives 100 USD, if two pips appear she will get 200 USD, and so on. Assuming the dice is fair, her expected pay-off from this game will be 350 USD. If the second alternative would be a guaranteed payment of 350 USD, would the investor be willing to take the gamble and roll the dice?

\nomenclature{USD}{US-Dollar}

Most (behavioural) finance research would answer this question with \enquote{no}, with the main argument being similar to: the \textit{rational} investor does not only care about the average pay-off she will receive (which would be the same in both alternative choices of the game), she is also interested in the riskiness as to which this pay-off will occur. Assuming that the average investor will perceive the expected deviation from the mean payment as \enquote{riskiness} (or as it shall be called henceforth: \enquote{volatility}), the second option is clearly preferred; the monetary outcome is secured and is not connected to any uncertainty about the payment of the game whatsoever\footnote{In this game, the first alternative (gamble) exhibits a standard deviation of around 171 USD, while the second alternative (safe payment) has a standard deviation of zero.}. In other words, as \textcite[77]{Markowitz1952} pointed out in his pioneering paper about portfolio theory: \enquote{the investor does (or should) consider expected return a desirable thing and variance of return an undesirable thing.}

Being a variable clearly of interest to investors, the task of forecasting the volatility of financial time series experienced a true \enquote{boom} in the past few decades, in both theoretical as well as empirical research, especially after the publication of two seminal papers by \textcite{Engle1982} and \textcite{Bollerslev1986}, who introduced the (G)ARCH models and were the first ones to model conditional variance series as time series. And also practitioners acknowledge the importance of return variance in real-world financial applications, the most famous probably being derivatives pricing, portfolio selection, and risk management. The latter also increased awareness of regulators with regards to monitoring asset volatility.

Moreover, volatility has become to be considered an \enquote{asset} per se, in the sense that one can speculate on how it will develop in the future. The most famous means to invest in volatility as an asset is using index-tracking instruments which mimick the Chicago Board Options Exchange SPX Volatility Index (usually referred to as VIX). The VIX provides estimates about general market volatility by combining option-implied volatilities at different strikes for single stocks and aggregating them in a weighted-average method. A very recent real-world example demonstrates the importance in an accurate prediction of volatility for investment purposes: in early February 2018, the VIX increased by more than an unprecedented 100 per cent in a single day. Some exchange-traded products, such as the \enquote{Inverse VIX Short-Term exchange-traded note}, issued by Swiss bank Credit Suisse and accordingly named XIV, who bet on the “calmness” of markets, lost a substantial amount of value due to the spike in market volatility. The XIV, for instance, closed almost 80 percent down, leaving the issuer of the securities to experience a siginficant 8.5 percent drop in its share price the subsequent day \parencite{CNBC2018}.

\nomenclature{VIX}{Chicago Board Options Exchange SPX Volatility Index}
\nomenclature{XIV}{Inverse VIX Short-Term Exchange-Traded Note}

Such examples raise the question on what the drivers of an financial asset's volatility are. Focussing on public equity markets, many factors have been used in many models in the attempt to explain volatility. The most \enquote{powerful} proved to be past realizations of volatility; a fact that well describes the empirically observed auto-regressive nature of return variance. It is indeed a \enquote{stylized fact} that volatility tends to occur in so-called \emph{clusters}. This circumstance also strongly connects volatility modelling with time series analysis and forecasting, with the main workhorse being the usage of lagged observations of variance in order to explain it \enquote{today} (and in the future, as well). 
Besides past return series, other authors (such as \textcite{Paye_VolaMacro_2012}) have attempted to add further financial and macroeconomic factors in their predictions (e.g., GDP fluctuations or inflation as explanatory indicators); however, there is little consensus in the literature about the usefulness of such variables in volatility modelling \parencite[2]{MittnikRobinzonovSpindler_2015}. Another driver of volatility has shown to be the so-called (option-price-) implied volatility for single stocks, or, on an aggregate level, the VIX for market-level volatility (cf. \textcite{MittnikRobinzonovSpindler_2015} and referenced literature therein). 

More recently, some authors have also attempted to include what can be called \enquote{soft} or \enquote{qualitative} information in their (volatility) models; very often such variables are used in addition to quantitative data (such as historical time series in prices, returns, or volumes, financial statement and balance sheet metrics, etc.). The latter are \enquote{hard} inputs in the sense that they are observable and verifiable, while qualitative information by nature is more subjective and imprecise \parencite{LM-meta-2016}. The largest part of such soft information content is available in the form of text. However, as \textcite[146]{Das2014} points out, \enquote{until recently, financial analysis was just based on numbers. Usage of text required human coding of attributes into numerical form before yielding to analysis}. Having available computer machines with larger storage capacity, faster processing power and appropriate software to analyze textual inputs, the applications of text mining and processing in the domain of finance and accounting have increased tremendously since the 1990s \parencite{KumarRavi2016}. Representatively of this rising attention towards textual information, well-known economist Hal Varian, when reviewing the paper of \textcite{AntweilerFrank2004} in his New York Times column in 2004, stated: 
\SetBlockThreshold{1}  
\blockquote[\textcite{Varian-NYT-2004}]{In the 1970's we saw the rise of Wall Street quantitative analysts. Then came program trading. Perhaps computational linguistics and textual data mining will become the new hot technologies in financial economics.}

While the terminology of \enquote{text mining} is often used interchangeably with phrases such as computational linguistics, content analysis, natural or statistical language processing, text analytics, information retrireval, stylometrics, etc. \parencite{LM-meta-2016}, the basic concept for the purposes of this thesis can be defined as follows: 
\SetBlockThreshold{1} 
\blockquote[\textcite{Das2014}]{Text mining is the large-scale, automated processing of plain text language in digital form to extract data that is converted into useful quantitative or qualitative information.}

Referring to a meta-study by \textcite{KumarRavi2016}, the applications of text mining techniques within the field of research in the business and economics domain are broad; specifically, they categorize them very broadly into four buckets: \enquote{FOREX rate prediction, stock market prediction, customer relationship management (CRM) and cyber security} \parencite[128]{KumarRavi2016}. However, text mining in the business domain is no longer a purely academic undergoing; real-world applications, for instance, can include the measurement of customer satisfaction and churn likelihood by mining unstructured reviews of clients, the development of early-warning systems for banks that incorporate sentiment information in estimation of default probabilities \parencite{McKinsey_2013}, or the establishment of an internal communication and monitoring tool for companies so to measure employee mood, motivation, or satisfaction \parencite{RMM_2015}. Moreover, the range of applications is very likely to increase in the future, as \enquote{it's not hard to see how almost any business could eventually reap rewards from the ability to comb through the writings of millions of people to identify coming desires and/or needs in entertainment, food, travel, retail -- pretty much anything, in both the consumer and B2B space} \parencite{TIME_2012}. 

Furthermore, irrespective of the actual application framework, the universe of textual sources which can be tapped is very large; a non-exhaustive list might range from brokerage/analyst reports, minutes of board or committee meetings, \textbf{10-K reports} or other filings and prospectuses with the SEC, corporate announcements, customer reviews and forum posts, news articles, social media and blog posts, e-mails, conference call logs, and many more. 10-K* filings of US-companies with the United States Securities and Exchange Commission (henceforth abbreviated using the common acronym SEC), highlighted in bold face, will be the textual corpus under consideration in this thesis\footnote{\label{fn: 10K-def}I will henceforth use the *-notation to denote all variants of 10-K filings. The filing types occurring in the sample of this thesis encompass the following: 10-K, 10-K-A, 10-K405, 10-K405-A, 10-KSB, 10-KSB-A, 10-KT, 10-KT-A, 10KSB, 10KSB-A, 10KSB40, 10KSB40-A, 10KT405 and 10KT405-A. The filing variants 10-K405, 10-K405-A, 10KSB40, 10KSB40-A, 10KT405 and 10KT405-A reports were only allowed up till 2003 and indicate a check-the-box rule (regarding the so-called Regulation S-K or S-B Item 405). KSB reports were eligible for small businesses up till 2009. The interjectional letter \texttt{T} indicates transition reports, while addendum \texttt{-A} indicates amendments to previously filed reports. For details regarding 10-K types, refer to \url{https://help.edgar-online.com/edgar/formtypes.asp} (accessed on 07/30/2018).}, with the final sample containing a rich collection of 46,483 filings from the time period 1999-2017. However, having at hand such a large amount of textual input might bring up problems and obstacles on its own. In this context, \textcite{McKinsey_2013} point out from a practitioner's point of view:
\blockquote{Enormous quantities of textual information are available \textelp{} and it is notoriously difficult to use. \textelp{} The challenges of mining this information and separating the signal from the noise are substantial. \textelp{} A database with news articles on about 1,000 companies easily exceeds 20 GB, orders of magnitude more than a financial database on these companies. Storing this much data is not difficult, but any kind of statistical analysis becomes an \enquote{overnight job}, even with optimized algorithms and systems. \textelp{} Second, textual data are unstructured. While it is relatively easy to analyze financial data in a statistical way — figures become meaningful at a certain size and in relation to sample averages — texts are a priori meaningless to a computer. There are no standard or statistical procedures for a machine to analyze and interpret texts. \textelp{} Third, texts are often ambiguous. \textelp{} In fact, almost all the semantic difficulties of written language pose immense problems for machines.} 

\nomenclature{SEC}{(United States) Securities and Exchange Commission}

When it comes to the techniques available for analyzing textual contents for financial purposes, a lot of methodologies are available, while the most common stream of activity in this domain can be subsumed under the umbrella of \enquote{sentiment analysis}. \textcite{RMM_2015} defines it as follows:
\SetBlockThreshold{1}   
\blockquote{Sentiment analysis studies the mood, opinions and attitudes expressed in written text. It aims to discover the emotions behind words in order to determine whether a communication suggests a positive, negative or neutral sentiment.}

It is the goal of this thesis to combine volatility forecasting with large-scale textual analysis; more specifically, the textual sentiment embedded in 10-K* filings will be extracted in an innovative fashion (sentiment term weighting on the basis of past volatility impact) and then used as an additional explanatory factor in the attempt to predict realized variance after the filing of annual reports. This work thereby contributes to existing literature in two dimensions: research connected to the field of volatility prediction and forecasting as well as textual analysis in finance and accounting. With respect to the first stream of literature, a cross-sectional analysis of 46,483 corporate 10-K filings brings to light that positive and negative sentiment embedded in the reports add incremental value in predicting post-filing realized volatility above a simple past predictor (pre-filing realized volatility) as well as conventional time-series models from the GARCH family. Conversely, more granulated linguistic aspects such as assertiveness, uncertainty, litigiousness, readability, or usage of financial keywords, provide little value added in explaining post-filing realized volatility. The second contribution of this thesis concerns language analysis in the empirical finance domain. In this aspect, a new term-weighting method was introduced: instead of using raw term frequencies or established corpus-related weighting schemes, I applied a market-based methodology, smoothing term counts based on their contribution in explaining volatility after past 10-K filings. Most findings revealed in this work are robust across multiple dimensions, such as choice of training/test sample split, alternative volatility proxies, or selection of term-weighting scheme. One special side-note that was discovered in robustness checks further sets this work in contrast to research results related to equity returns: as was revealed in an extensive analysis, volatility models from the GARCH-family seem to do an accurate job in forecasting, while such precision is not present in models for return series. Thus, textual analysis on return series is leaving \enquote{more room for incremental improvement} coming from qualitative and potentially noisy input features such as text tonality and sentiment.

The remainder of this thesis is structured as follows: section \ref{sec: lit_rev} provides an overview about existing literature and theoretical foundations of volatility forecasting and textual analysis in the finance and accounting domain. Section \ref{sec: sentiment_calcs} provides an overview of how 10-K* filings can be mined in a vector-space framework and in this context also presents the first block of my research framework, the methodology of volatility-impact-based term weighting. Section \ref{sec: volamodel} introduces the second pillar of the research design, i.e., presents the hypotheses which connect the document sentiment scores (and other control variables) with post-filing volatility and describes the econometric model used. Section \ref{sec: data_sample} gives insights about the data collection and cleaning procedures as well as statistical descriptions about the final sample. Section \ref{sec: results} provides the uni- and multivariate estimations and a discussion of the results. Section \ref{sec: robustness} offers some thoughts and examinations regarding result robustness. Section \ref{sec: conclusion} draws final conclusions and offers impulses for future research, especially with regards to alternative research design specifications.
\clearpage