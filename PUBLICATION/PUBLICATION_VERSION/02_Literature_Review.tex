\section{Literature Review and Theoretical Background}
\label{sec: lit_rev}

In this section I will provide an overview about influential literature in the field of textual analysis in finance and accounting, with a particular focus on contributions focussing on sentiment analysis. Moreover, I will elaborate on publications related to the readability of financial documents. Finally, a short summary about literature that uses corporate filings for text analysis purposes is provided. 

% ----------------------------------------------------------- %

\subsection{Text Mining and Natural Language Processing in Finance and Accounting}
\label{ssec: lit_rev_TM_NLP}

As \textcite{YukselturkTucker2015} evidence, research based on content analysis in the areas of accounting and finance has gained significant importance recently, with growing literature available in the fields of corporate disclosure and financial reporting as well as analyst reports; in the same context they also report a \enquote{growing body of research on the impact of tone or sentiment on asset prices and returns} \parencite[871]{YukselturkTucker2015}. 
Excellent meta-studies on the use of textual analysis within finance and accounting are given in \textcite{KearneyLiu2014}\footnote{Especially their Table 1 and Table 3 provide a good overview of influential contributions, grouped by the source of textual inputs used, and the main findings, respectively \parencite[3, 12]{KearneyLiu2014}.}, \textcite{GuoShiTu2017} as well as \textcite{LM-meta-2016}. As the latter point out, the idea of mining textual contents to search for patterns and information has a long tradition and reaches far back into history, ranging from biblical keyword analysis to the dissection of the rhetorical content in political speeches. In more recent times, both large increases in computing power as well as availability of textual material have also encouraged researchers from the finance and accounting disciplines to apply textual analysis in these domains. \textcite[1188]{LM-meta-2016}, with regard to this trend,  trenchantly point out that \enquote{the online availability of news articles, earnings conference calls, Securities and Exchange Commission (SEC) filings, and text from social media provide ample fodder for applying the technology.}  

A generic research design of finance and accounting studies which encompass textual analysis is schematically depicted in Figure \ref{fig: process_schema}: starting with the textual \enquote{underlying}, one usually seeks to extract some output from a collection of documents (often referred to as \textit{corpus}). The desired outputs are various (such as sentiment scores, readability measures, similarity metrics, topic categories, document rankings, and many more); likewise, the methods to extract such variables from text are manifold and can include counting the appearance of certain keywords or chain of words (so called \textit{n-grams}), semantic decompositions, the classification of sentences, paragraphs, or documents, or more sophisticated machine learning algorithms such as support vector methods or neural networks. These text-related outputs are then usually connected with quantitative information, whereas tools range from established statistical and econometric procedures (mostly linear or logistic regression) to more advanced fusion methods that involve algorithms from machine learning.

In general, however, natural language processing in the financial domain brings up several obstacles and difficulties for the researcher. A fundamental question is related to an essential trade-off between signal and noise when choosing the research design; in other words, although analyses sometimes use simplifying assumptions\footnote{Most often this assumption reflects the idea that documents can be represented as so-called bags of words which allow the researcher to conduct an analysis on token-level, while at the same time this approach assumes that the ordering of words within a documents is irrelevant. I will further elaborate on this assumption in section \ref{ssec: senti_counts}, when I introduce the vector space model that will be the starting point for the analysis in this thesis.}, the attempt to better capture meaning and context also from a syntactic and semantic point of view can in some cases do more harm than good, because one simply adds more noise to the model that shall not be mistaken for signal \parencite{LM-meta-2016}. 

Further issues, which affect a lot of studies, are data availability, retrieval issues such as downloadability, document structure, and the subsequent parsing process. For some corpora, no collections or databases are available and require download by hand. Moreover, the downloaded files are often not available in the required format or are not machine-readable\footnote{In most cases, the property of being machine-readable is a minimum requirement for any corpus, so this is a rather theoretical side-note.}, or require at least a minimum amount of parsing. This imposes large practical problems, as \enquote{document parsing relies on consistency in the structure of the text and any related markup language} \parencite[1192]{LM-meta-2016}. This requirement is not always given in real-life applications, where data is often unstructured or inconsistent (over time)\footnote{One such issue will be further explained in the description of the corpus of 10-K* filings used in this thesis (see section \ref{sec: data_sample} about data collection and, specifically, footnote \ref{fn: parsing_LM})}. Other problems connected to the parsing procedure arise at the stage of tokenization or sentence-/paragraph-level segmentation. In this context, \textcite[1215]{LM-meta-2016} provide a good introductory overview of challenges and tripwires when applying natural language processing in the financial domain. In particular, the authors highlight issues which arise from what might sound like an obvious statement, namely that\enquote{all textual methods are based on first identifying words}. However, the undergoing of identifying \textit{words} is more subtle than it appears at first sight. One needs to clarify what happens, for instance, with compound words that are connected via hyphens (e.g., \textsf{well-known} or \textsf{up-to-date}), proper nouns (e.g., the \textsf{Fama} and \textsf{French} from the Fama-French-Model), and abbreviations (e.g., \textsf{FC} for football club or \textsf{Mr.} for Mister). Moreover, the example chosen for proper nouns reveals yet another problem: how can one deal with polysemy, i.e., the fact that the surname \textsf{French} (of famous economist Kenneth French) looks fully equivalent to the description of nationality (\enquote{to be of \textsf{French} origin/nationality}), yet the two words have very different meanings? This would require the researcher or the machine to learn from context, which in turn calls for usage of constructs that go above the granularity of a word-based analysis. For instance, this could be learned using n-grams (recognizing that the name \textsf{French} will likely tend to co-occur with words like \textsf{Mr.}, \textsf{Fama}, \textsf{Model}, etc.; whereas the nation-based version will rather co-occur with \textsf{British}, \textsf{German}, and so on). Furthermore, conjugations and declinations can impose problems: words like \textsf{calculate, calculates, calculated, calculating}, etc. all refer to the same word stem, yet are treated differently just because of their suffix. As will described in section \ref{ssec: senti_counts}, lemmatization and stemming deal with that exact problem -- in most cases by chopping off the endings and summing up those four instances as one single token (\textsf{calculat}). 

Similarly, also an apparently trivial task like splitting a document into sentences can prove to be tricky in practice. Generally, the parsing algorithm needs to be instructed on how to treat common sentence delimiters such as periods (\textsf{.}), colons (\textsf{;}), question marks (\textsf{?}), or exclamation marks (\textsf{!}). As \textcite[1216]{LM-meta-2016} evidence, the usage of \enquote{extensive lists, technical terminology, and other formatting complexities, makes sentence disambiguation especially challenging in accounting disclosures}. An illustrative example of this challenge would be the treatment of section headers such as \textsf{2.1. Financial Statements} or decimal separators as in \textsf{199.99 USD}. An inaccurate parser will -- based on the presence of the period sign -- mistakenly split the document at these points; and \enquote{the parsing errors in this case can be extraordinary}, implying that \enquote{Generic sentence parsing algorithms do not work well on financial documents} \parencite[1216, 1217]{LM-meta-2016}. This imprecision has crucial impact on the variable construction, implying the latter are therefore inherently measured with noise. For instance, most document readability measures are constructed using the number of sentences in a document; and thus critically depend on how accurately sentences can be identified.

\subsubsection{Readability}
\label{sssec: lit_rev_mining_readability}
Pioneering contributions in the stream of literature connected to readability of finance- and accounting corpora are \textcite{Li2008}, who evidenced that firms with lower current earnings (or earnings decreases) on average tend to produce less readable as well as longer 10-K filings\footnote{The argument provided suggests that management has an inclination to produce longer and more complex 10-K filings so as to diffuse or dilute bad news to investors. \textcite[221]{Li2008} labels this phenomenon \enquote{management obfuscation hypothesis}.}, and \textcite{DeFranco2015}. While the former \enquote{suggests a clear correlation between the linguistic features of annual reports and firm performance} \parencite[222]{Li2008}, \textcite{DeFranco2015} investigate the readability of security analysts' research reports. They find report readability to positively influence stock trading volume\footnote{A potential explanation for this reads as follows: if a report, ceteris paribus, classifies as \enquote{more readable}, the information conveyed in the document will be perceived as more precise information and thus trigger stronger trading signals for investors \parencite{DeFranco2015}.}, with the latter being a commonly used predictor for stock return volatility and thus relating to this thesis indirectly.

In a similar setting, \textcite{HsiehHuiZhang2016} relate stock price reactions to the degree of readability upon publication of analyst reports. They find that the cumulative abnormal return (CAR) in a $[-1,1]$-day window around the report issuance date increases by 58 basis points if the readability of the report increases by one standard deviation. A particularly important side note for this thesis is the statistical significance that \textcite{HsiehHuiZhang2016} as well as \textcite{DeFranco2015} find for one of their control variables, namely the narrative tone that analysts use -- thus indicating that not only readability but also textual sentiment matters for the market in determining price and volume for equity instruments\footnote{\textcite{DeFranco2015} additionally include an interaction term between readability and tone, which they find to be positive and significant. This suggests that these two variables reinforce each other.}.

% COULD INCLUDE THIS INTERACTION VARIABLE AS WELL .....

\nomenclature{CAR}{Cumulative Abnormal Return}

In the area of volatility prediction based on readability of financial documents, the most relevant paper for this thesis is \textcite{Loughran2014}. They evidence that volatility significantly decreases with the degree of readability of 10-K reports. Suggesting another, more indirect, channel of cause and effect in this context, \textcite{LehavyLiMerkley2011} have shown that less readable 10-K filings tend to be connected with increased analyst coverage\footnote{The authors provide a potential explanation for that phenomenon: a 10-K that is relatively difficult to read has too high processing costs for a single analyst to cover them. Thus, naturally, in order to meet investor's information demand, more analysts will start to track the stock and exert collective effort.}. Referring to the findings of \textcite{Schutte2007}, who evidence that analyst coverage initiation helps investors to better distinguish \enquote{true} firm-specific information from pure noise signals, it can be inferred that the fact that an analyst tracks the company \textit{per se} has a negative impact on firm-specific volatility. This interpretation undermines the findings of \textcite{Loughran2014}, namely that more readability in 10-K's (be it directly or via increased analyst coverage) reduces the stock price volatility of the companies submitting the filings.

Regarding the \textbf{measurement} of readability, different metrics have evolved in the literature, with the most commonly used being the Gunning-Fog-Index as well as the Flesch-Kincaid-Grade-Level-Formula and the Flesch Reading-Ease Score (FRES)\footnote{All of these measures effectively are a linear combination of two components, namely the average length of sentences as well as proportion of \enquote{complex} words used, while complexity of words is determined by syllabication. A more detailed definition of these metrics is provided in appendix \ref{sec: annex_fogflesch}.}; nevertheless, \textcite[1649]{Loughran2014} argue that these measures, because of their strong focus on sentence length and word complexity, are not suitable in the context of business-related text. Moreover, they do not adequately account for differences in background knowledge between the targeted audiences. In fact, based on the Fog measure, very common words like \textsf{company}, \textsf{management}, or \textsf{financial} would be considered to be \enquote{complex} words -- yet they are still very likely to be understood by the average investor. Furthermore, all of the common readability measures require some form of text parsing and are thus exposed to typical inaccuracies and subjectivities connected to such procedures. In the work of \textcite{Loughran2014}, the \enquote{underlying} textual source were 10-K filings with the SEC. For these documents the authors suggest an alternative that is a much simpler, less error-prone and more reproducible metric, yet is still correlated to \enquote{conventional} measures like the Fog Index and equally able to capture readability: the natural logarithm of gross file size (measured in megabytes) of the 10-K filing\footnote{One should note at this stage that this approach requires the researcher to account for firm complexity (most likely using a proxy like firm size), as longer reports could be simply due to the complexity of the fundamental business \parencite{LM-meta-2016}. }.

\nomenclature{FRES}{Flesch Reading-Ease Score}

\subsubsection{Sentiment Analysis}
\label{sssec: lit_rev_mining_senti}

As \textcite{LM-meta-2016} indicate, the most simple approach to a textual analysis with financial corpora is to search the documents for appearance of certain keywords (or keyword phrases) of interest (for instance, they cite phrases like \textsf{ethic} or \textsf{corporate social responsibility}, whose frequency can be used when one attempts to quantify corporate governance metrics or the probability of lawsuits). A natural extension of this methodology involves to not look for specific phrases only, but rather use a dictionary of words\footnote{Throughout this thesis the words lexicon, dictionary, or (sentiment) word list will be used interchangeably; although from a linguistic and literary point of view they are not equivalent.}, all of which carry similar characteristics. In the large majority of cases these word lists are chosen so as to share common sentiment and thus relate directly to the technique of sentiment analysis. 

The frequency of sentiment words is then often aggregated into a scalar, constructing what is often referred to as \enquote{sentiment index} or \enquote{sentiment score}\footnote{Note that the strong focus on keyword counting in sentiment analysis has lead the discipline to be dominated by vector space models (or, equivalently, term-document-matrices) within the bag-of-words framework (see section \ref{ssec: senti_counts} where these concepts will be expanded in the process of introducing the research design applied in this thesis.}. Apart from the usability as a variable in many research applications, sentiment indices are common also in practice, as the following quote undermines:
\blockquote[\textcite{McKinsey_2013}]{Gauging sentiment with an index makes it possible for machines to analyse the information; it can be converted, aggregated, and compared. And the index can be used with statistical analysis to build prediction models. Obviously, the difficulties come in the details of assigning the sentiment index. At the core of the process is a lexicon that lists words or phrases that represent a certain kind of sentiment and, importantly, reflect the specific context in which the text appears.}

The last part of the definition highlights a critical fact that distinguishes sentiment analysis in finance and accounting from other disciplines: while early studies used \enquote{generic} lexica\footnote{The most prominent choice for word classification among the heap of lexica available was the Harvard General Inquirer (GI) (in particular, the Harvard IV-4 dictionary, see \url{http://www.wjh.harvard.edu/~inquirer/homecat.htm}).} that were developed from (and thus suitable for) texts in psychology and sociology, \textcite{Loughran2011} were the first to recognize that those lists are not applicable to documents in the business domain. In fact, using 10-K filings, the authors find that almost three quarters of allegedly negative words in the Harvard IV-4 list do not appear to be negative in business context\footnote{For instance, such words are \textsf{tax, cost, capital, board, liability, foreign}, and \textsf{vice}, which appear very often in 10-K filings, yet are very likely not negatively connoted \parencite[36]{Loughran2011}.}. Therefore, \textcite{Loughran2011} created finance-specific sentiment word lists in the following categories: negative, positive, uncertainty, constraining, litigious, strong/modest/weak modal. These lists (especially the negative lexicon) have become the \enquote{gold standard} in financial sentiment analysis and are also the backbone of this thesis. Henceforth, I will label these word lists using the acronym LM dictionary (or lexicon/word list). 

\nomenclature{LM (Dictionaries)}{Sentiment Dictionaries developed in \textcite{Loughran2011}}

Turning from this methodological introduction to relevant literature contributions, a good starting point for dictionary-based sentiment extraction is the paper of \textcite{YukselturkTucker2015}. They calculate thematic net sentiment scores for a UK-based sample of analyst research reports by classifying (80 to 180) keywords contained in six theme-buckets\footnote{The six themes are the following: macroeconomic and regulatory environment, industry and market environment, growth, management and strategy, financial performance, financial position.} into positive and negative keywords, while the classification task was performed using both Harvard IV-4 and LM dictionaries. The main finding was that theme-related (net) sentiment affects the two key outputs (recommendations and target prices) of the research analyst. \textcite{HuangZangZheng14} extended this stream of research regarding textual contents of analyst reports. They quantified sentiment of sentences contained in analyst reports using a Naive Bayes machine-learning algorithm. After controlling for a simultaneous change in the quantitative summary metrics (i.e., price target, stock recommendation and EPS forecast), they show that the textual opinion embedded in the respective analyst report helps to explain abnormal stock returns subsequent to the release of the report. Moreover, they evidence that the narrative content of analyst reports also carries \enquote {predictive value for future earnings growth in the subsequent five years} \parencite[2151]{HuangZangZheng14}. 

In the same category of research falls also an important paper of \textcite{Tetlock2007}. He mined a popular column in the \textit{Wall Street Journal} in order to gauge market sentiment; in particular, he constructed a pessimism index using the Harvard GI lexicon, which was shown to be predictive for lower subsequent returns and higher volatility.

Another pioneering paper in this area of research, which due to its nexus to volatility prediction is also relevant for this work, is \textcite{AntweilerFrank2004}. They analyzed more than 1.5 million messages from message boards about 45 companies; using the Naive Bayes approach to classify messages to either buy, hold, or sell category. The single classified messages were then aggregated into a bullishness as well as an agreement index, which proved to be successful in predicting both trading volume and volatility, but unsuccessful in explaining stock returns. Regarding volatility, only the bullishness seems to carry explanatory power while the correlation is positive (i.e., more bullish messages today imply greater volatility tomorrow), while agreement between message posters does not help to predict realized volatility. 

Most importantly, however, this thesis builds upon the work in \textcite{Kogan2009_1}, which was extended in \textcite{TsaiWang2012, TsaiWang2013, TsaiWang2014, TsaiWangChien2016, WangTsaiLiuChang2013} and \textcite{TsaiWang2016}; all of which essentially show that textual sentiment of 10-K's helps to explain stock return volatility in the quarter/year after the filing (either in a ranking or regression framework). These results were yet further refined in \textcite{Rekabsaz2017}, who used a more elaborate word weighting scheme on the LM sentiment lists and also investigated on potential industry-specific patterns. 

\subsubsection{Text Analysis Using 10-K Filings Corpora}
\label{sssec: lit_rev_mining_10K}

As it is evident from the literature review up to this point, corporate filings as a mandatorily disclosed document provide a rich source of potential research questions in the textual analytics field and are very popular within the finance and accounting domain. In this context, \textcite[223]{Das2014} points out\footnote{In that regard, \textcite[3]{KearneyLiu2014} highlight that while corporate disclosures provide \enquote{a natural source of textual sentiment for researchers insofar as they are official releases}, their main disadvantage \enquote{is the low frequency of the data, because firms usually make these disclosures on a quarterly or annual basis.}}: 
\blockquote{Whereas much of financial text analysis uses messages posted to finance boards like Yahoo!, Motley Fool, or to blogs such as Twitter, Facebook, etc., other textual analysis has been applied to company reports and filings. In the former case, analysis is usually of a time series nature, whereas in the latter, text analysis is undertaken across companies in a cross-section.}

Regarding the heap of corporate filings available, \textcite[227]{Das2014} further highlights that most of the literature is focussing on 10-K filings because these are considered to be the most informative document for (potential) investors\footnote{I follow this stream of literature and use a broad corpus of 10-K*s (refer to footnote \ref{fn: 10K-def} for a description of variants of the \enquote{standard} 10-K). Moreover, to avoid permanent repetition I will use the terms 10-K, filing, or (annual) report synonymously, although the SEC highlights that \enquote{10-K typically includes more detailed information than the annual report to shareholders.} (see \url{https://www.sec.gov/fast-answers/answersreada10khtm.html}, visited on 05/27/2018)}. 10-K reports are mandatory disclosure documents that are to be filed annually with the SEC. As it is outlined on their web-page\footnote{See \url{https://www.sec.gov/fast-answers/answersreada10khtm.html}, accessed on 07/31/2018.}, the report's contents contain an important source of information and decision-making material for investors: \enquote{Among other things, the 10-K offers a detailed picture of a company's business, the risks it faces, and the operating and financial results for the fiscal year. Company management also discusses its perspective on the business results and what is driving them.} As \textcite{Pulliza2015} points out and what is of direct importance for all language analytics studies using this corpus, 10-K filings shall be conform with the SEC Plain English Rule, which was established with the target that communication from the management towards the investor community is written in plain and understandable terms without an overload of complexity.

One should note that this part of the literature review reflects only \enquote{related} contributions, i.e., publications with a focus on natural language processing and, more specifically, sentiment analysis. The (much broader) stream of literature that deals with the \enquote{quantitative} outputs of (mandatory) corporate disclosure and the information content as well as the market reactions of such disclosures, is not part of this review. This, for instance, encompasses a large range of publications in the field of accounting policies (accounting standards, international comparisons, etc.) and manipulation methods (with contributions related to (abnormal) accruals mainly) as well as mergers and acquisitions including the subsequent purchase price allocations (with contributions related to intangible assets, goodwill, etc.) that are disclosed in annual filings. In fact, I view the combination of such \enquote{hard} information content with the textual tone as a much desired extension for further research; a fact on which I will further elaborate in the conclusions of this thesis (cf. section \ref{sec: conclusion}). 

One of the first questions to ask when analyzing 10-K filings is whether to text mine the whole document or rather target a specific section in it. The most famous of them, which is often said to be the most informative part and carry the most forward-looking statements, is Item 7 of the 10-K filing, which is referred to as \textit{Management Discussion and Analysis} (MD\&A). For instance, this section is the subject of study in widely cited papers like \textcite{Kogan2009_1, Li2010, Feldman_et_al_2010, TsaiWang2012, WangTsaiLiuChang2013, TsaiWang2013, TsaiWangChien2016, TsaiWang2016}. 

Another common choice is \textit{Item 1A - Risk Factors}, \enquote{which contains information about the most significant risks for the company} \parencite[1712]{Rekabsaz2017}, which is used, among others, in \textcite{HuangLi2011} and \textcite{Rekabsaz2017}. A good overview of all sections within a 10-K filing is provided on the SEC web-page\footnote{See \url{https://www.sec.gov/fast-answers/answersreada10khtm.html}, accessed on 7/31/2018}.  

However, also section-based analysis comes with obstacles: as \textcite{LM-meta-2016} evidence, parsing algorithms struggle with the fact that some 10-K submissions are unstructured (especially before 2002), some sections are mislabelled (e.g., MD\&A is falsely tagged as Item 6 instead of 7), or that companies place content across different parts of the document and cross-reference across the filing using footnotes. Regarding the latter, \textcite{HeidariFelden_Footnotes_2015} presented a promising idea, that might be an interesting extension to 10-K corpus based research: they tried to categorize income tax footnotes within 10-K and 10-Q's into pre-defined buckets and proved that machine-based classifiers can achieve the same task with relatively high accuracy, allowing the researcher to extract content from an \enquote{unstructured} part of the filing without the necessity of reading the footnotes manually. This was extended upon by \textcite{Amel-Zadeh_Faasse_2016} and \textcite{ThinggaardJeppersenMadsen2016}, who compared MD\&A content with footnote disclosure, the latter doing so for the Danish market and therefore belonging to the absolute minority of publications which use non-US based data.

The most influential papers who focus on the 10-K as single, homogeneous document are \textcite{KothariLiShort_2009, LehavyLiMerkley2011, Loughran2011, Loughran2014, LM-meta-2016}, while good introductory overviews about 10-K language processing (and, more generally, textual analysis in finance and accounting) are provided by \textcite{Qiu07, Pulliza2015, LM-meta-2016}. 

\nomenclature{MD\&A}{Management Discussion and Analysis (Section within a 10-K Filing)}

\clearpage